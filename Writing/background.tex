\documentclass{report}
\input{commands.tex}

\begin{document}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\input{introduction.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BACKGROUND MT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{Ch2.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BACKGROUND EMPIRICAL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
%\input{Ch3.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5 
% OWN WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
%\input{Ch4.tex} 


\section{Structures for Language}

The monolingual assumption implicit in transfer models, is that structural representations of natural language sentences exist, from which the meaning can be derived. In other words, it is assumed that there is an underlying system, according to which sentences can be assigned structures (a process often called `parsing'). Such a system can be seen as a grammar for the language. There is no consensus in linguistics as to the existence of such a system. On the one hand, there is the Chomskian group of researchers, that advocate the existence of an underlying compositional grammar universal to all human beings \citep[As first claimed in][]{chomsky1956three}, while others believe no such system exist, and language users rely on some sense of familiarity with what they have heard before \citep[e.g.,][]{scha1990taaltheorie, frank2012hierarchical}. However, cognitive considerations are beyond the scope of this thesis, and we will not further discuss this debate. 
%say what we WILL do in this section


\subsection{Considerations about Monolingual Compositionality}

The stand that language is a compositional system generated by grammar is well discussed in the literature. The following theoretical principle captures this property of language:

\begin{quote}
\textbf{The Principle of Compositionality}\\
The meaning of an expression is a function of the meaning of its parts and the syntactic rule by which they are combined \citep{partee1984compositionality}
\end{quote}

The principle of compositionality applies rather obviously to (most) artificial languages created by humans. The meaning of their expressions can be unambiguously determined by considering the atoms and the rules used to combine them. For instance, consider the expression $p\lor q$ in propositional logic. Its truth-value can be determined by plugging in the truth values of $p$ and $q$ into the rule of the $\lor$-connective. Also programming languages can be interpreted by considering their basic units and the rules used to combine them.

Intuitively, it seems very reasonable that natural languages should obey this principle as well: we do not store the meaning of all possible sentences of language in our head, and yet we have no trouble understanding new sentences, presumably because we know the words in it, and the methods that can be used to combine them. Besides, we can all perceive a certain systematicity and recursion in the way sentences are constructed. This sytematicity is often referred to with the term `syntax'. However, it is hard to establish the level of compositionality of language as a whole. The difficulty consists of two, not unrelated, issues.

First of all, creating a grammar that covers \textit{all} grammatical utterances of a natural language, without generating too many ungrammatical ones, has proven a far from trivial task. It is surprisingly difficult to construct such a compositional grammar, even for a finite (but reasonably sized) non-trivial corpus \citep{scha1990taaltheorie}. The bigger the grammar grows, the more phenomena need to be taken into account, as well as how they interact with each other. Also, such a grammar should contain all idiomatic expressions of the language, whose meaning cannot be derived from its parts.

%Actually, this seems to be there in logical languages too, a and b and c is also 'ambiguous', read Scha again?
The second difficulty concerns ambiguity. In programming languages and logical languages, utterances typically have only one analysis, and their meaning is thus unambiguous. Under all current existing grammar formalisms assigning structures to natural language, all sentences (of some length) have many different structural analyses, of which usually only one or two are perceived by humans \citep{scha1990taaltheorie}. Even sentences that are considered unambiguous by humans thus need to be disambiguated.\footnote{Note that this problem differs from one of the standard counter arguments of compositionality, that concerns sentences like `two men carry two chairs', that \textit{are} considered ambiguous by humans, but cannot be assigned two distinct syntactic analyses capturing this difference \citep{pelletier1994principle}. Not particularly relevant, but certainly nice to notice, is that this type of ambiguity is not necessarily problematic for translation, as it might be preserved. For instance, the Dutch translation `twee mannen dragen twee stoelen' of aforementioned sentence has the same two meanings as the English one.} In practice, when working with compositional grammars, this issue is often solved by assigning probabilities to the grammar rules, and computing the structure with the maximum probability (or an approximation of this).

%I am not entirely sure what I should put here yet, some concluding remark, what is semantics, can we construct the semantics from syntax and the lexical items of the sentence? how large should these building blocks be? small enough to be still intuitively compositional? 
End paragraph with concluding remark


\subsection{In Practice}

We will now explain how compositional structures are generated, or identified. For the moment, we will not pay any attention to the syntactic rules used in the derivation of a structure, but merely focus on which parts were used during the construction of the sentence. This part of the thesis is very basic, but is very important for the work done in this thesis and will therefore be treated nevertheless.

Consider the simple sentence `I gave my little brother a new toy car.'. There are very many ways, in which this sentence could be built up from its parts, such as:\begin{itemize}
\item combine `brother' and `a' to get `brother a'
\item combine `I', `gave', `my' and `little' to get `I gave my little'
\item combine `new', `toy' and `car' into `new toy car'
\item combine `I gave my little' and `brother a' to get `I gave my little brother a'
\item combine `I gave my little brother a' and `new toy car' to get the entire sentence
\end{itemize}

This construction can be represented in a tree structure, as is depicted in Figure \ref{fig:struct1}.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\node (I) at (1,0.05) {I};
\node (gave) at (1.6,0) {gave};
\node (my) at (2.3,0) {my};
\node (little) at (3.0,0.07) {little};
\node (brother) at (4.1,.07) {brother};
\node (a) at (4.9,0.03) {a};
\node (new) at (5.4,0.03) {new};
\node (toy) at (6.1,0.02) {toy};
\node (car) at (6.7,0.02) {car};

\coordinate (Igavemylittle) at (2,0.7);
\coordinate (brothera) at (4.4,0.7);
\coordinate(newtoycar) at (6.0,0.7);
\coordinate (all) at (4.3,2);
\coordinate (Igavemybrothera) at (3.1, 1.2);

\foreach \from/\to in {Igavemylittle/I, Igavemylittle/gave, Igavemylittle/my, Igavemylittle/little, a/brothera, brother/brothera, newtoycar/new, newtoycar/toy, newtoycar/car, Igavemybrothera/Igavemylittle, Igavemybrothera/brothera, all/newtoycar, all/Igavemybrothera}
	\draw (\from) -- (\to);

\end{tikzpicture}
\caption{A tree that describe how the sentence 'I gave my little brother a new toy car' could have been compositionally constructed.}\label{fig:struct1}
\end{figure}

Formally, there is no reason to discard this structure, yet everyone that understands the English language will agree that this structure makes little sense, as we perceive that other parts somehow belong together, such as `my little brother', or `a new toy car'. The structures in Figure \ref{fig:struct2} therefore seem more plausible.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}

\node (I) at (0,0.05) {I};
\node (gave) at (0.6,0) {gave};
\node (my) at (1.3,0) {my};
\node (little) at (2.0,0.07) {little};
\node (brother) at (3.1,.07) {brother};
\node (a) at (3.9,0.03) {a};
\node (new) at (4.4,0.03) {new};
\node (toy) at (5.1,0.03) {toy};
\node (car) at (5.7,0.03) {car};

\coordinate (toycar) at (5.2,0.6);
\coordinate(newtoycar) at (4.9,0.9);
\coordinate (anewtoycar) at (4.7,1.1);
\coordinate (littlebrother) at (2.45,0.7);
\coordinate (mylittlebrother) at (2.2,1);
\coordinate (all) at (2.7,3);

\foreach \from/\to in {toycar/car, toycar/toy, newtoycar/toycar, newtoycar/new, anewtoycar/newtoycar, anewtoycar/a, littlebrother/brother, littlebrother/little, mylittlebrother/littlebrother, mylittlebrother/my, all/I, all/gave, all/mylittlebrother, all/anewtoycar}
	\draw (\from) -- (\to);

\node (I_) at (6.5,0.05) {I};
\node (gave_) at (7.1,0) {gave};
\node (my_) at (7.8,0) {my};
\node (little_) at (8.5,0.07) {little};
\node (brother_) at (9.6,.07) {brother};
\node (a_) at (10.4,0.03) {a};
\node (new_) at (10.9,0.03) {new};
\node (toy_) at (11.6,0.03) {toy};
\node (car_) at (12.2,0.03) {car};

\coordinate (toycar_) at (11.7,0.6);
\coordinate(newtoycar_) at (11.4,0.9);
\coordinate (anewtoycar_) at (11.2,1.1);
\coordinate (littlebrother_) at (8.95,0.7);
\coordinate (mylittlebrother_) at (8.6,0.9);
\coordinate (gavetobrother_) at (8.1,1.3);
\coordinate (gavetocar_) at (10,2.3);
\coordinate (all_) at (9.2,3);

\foreach \from/\to in {toycar_/car_, toycar_/toy_, newtoycar_/toycar_, newtoycar_/new_, anewtoycar_/newtoycar_, anewtoycar_/a_, littlebrother_/brother_, littlebrother_/little_, mylittlebrother_/littlebrother_, mylittlebrother_/my_, all_/I_, gavetocar_/gave_, gavetocar_/mylittlebrother_, gavetocar_/anewtoycar_, all_/gavetocar_}
	\draw (\from) -- (\to);
\end{tikzpicture}
\caption{Possible compositional structures for the sentence `I gave my little brother a new toy car'}\label{fig:struct2}
\end{figure}

Such tree representations can be used to capture any compositional structure: every node corresponds to the application of a syntactic rule in the derivation, where its children were the arguments. It is very common to assume that the nodes (corresponding to the parts) constitute contiguous sequences in the sentence, as such structures can be generated by context-free grammars. Sometimes, this gets in the way, as can be concluded from the tree depicted in Figure \ref{fig:struct3}, where the parts that naturally belong together are found in different places of the sentence (not that this structure is mathematically still a tree, the crossing arrows are only due to the fact that the leafs of the tree are not linearly ordered).

\begin{figure}[!ht]
\centering
\begin{tikzpicture}

\node (je) at (0,0.07) {Je};
\node (ne) at (0.5,0.04) {ne};
\node (fume) at (1.2,0.08) {fume};
\node (pas) at (2,0) {pas};

\coordinate (nepas) at (1.4,0.7);
\coordinate (jefume) at (0.6,0.7);
\coordinate (all) at (1.0,1.1);

\draw (nepas) -- (ne);
\draw (nepas) -- (pas);
\draw (je) -- (all);
\draw (fume) -- (all);
\draw (all) -- (nepas);

\end{tikzpicture}
\caption{'non-projective' tree}\label{fig:struct3}
\end{figure}

Without attaching any rules to the structures, it is impossibly to quantify coverage of the set of structures, or the easy with which sentences can be disambiguated. For applications or empirical research, labels thus need to be considered (or learned).

\section{Translation Structures}

Even when assuming compositional grammars for two languages exist, it is still questionable whether it is possible to find a mapping between the two grammars. Of course, in theory we could just create a mapping that maps entire source structures to entire target structures, in practice, however, we cannot.\footnote{And if this were possible, we might as well just create a mapping that maps entire source sentences to entire target sentences and be done with it.} A mapping should thus map parts of structures to parts of structures, and combine these parts into a new structure (yielding a compositional construction of the translation). This section is structured similarly to the previous one: we start with a theoretical discussion of mappings, which is followed by a discussion of how compositional translation manifests itself in practice.
 
\subsection{Theory}

As compositionality of language, compositionality of translation is described in a principle:

\begin{quote}
\textbf{The Principle of Compositionality of Translation}\\
Two expressions are each others translation if they are built up from parts which are each other's translation, by means of translation-equivalent rules. (ref??)
\end{quote}

In other words, compositional translation takes place by identifying the compositional structure of the target language, mapping the syntactic rules to translation equivalent rules on the source side, and recursively determining the translation equivalence of the parts. Compositional translation is a very common method in translation between artificial languages \citep{janssen1996compositionality,janssen1998algebraic}. The translation from one logical language into another, or the translation the compiler performs when interpreting a programming language are all compositional. When considering a purely semantical compositional grammar, it seems that compositionality of translation should also hold for natural language, yet it is unclear if translation can actually be executed this way. We will discuss the main issues.

An important assumption that is underpinning the principle, is that in translation not only meaning, but also form should be preserved (as much as possible). In other words, it assumes that translation is literal. For artificial languages this property is straight-forward and useful, mostly because there are no a priori reasons to prefer a non-literal translation over a literal translation. In natural language, this assumption is more questionable. There are many occasions in natural language in which the assumption seems fairly applicable. It captures, for instance, the fact that `all ravens are black' is an adequate translation of `alle raven zijn zwart', while the logical equivalent `if something is not black it is not a raven' is not \citep{landsbergen1989power}. However, in practice a translator can have many reasons to prefer a free translation even if a more literal alternative is also available. In MT-systems, this issue is often ignored, which does not seem like a huge concession, given that the these systems are not (yet) focussing on literary translations, and the most literal translation is often a reasonably good, or at least acceptable translation.

Syntactic and lexical translational divergences form a second problem for compositional translation. Languages do not always express the same set of meanings, or express meanings in the same way. Even in languages of cultures that are quite similar, one can find a number of words that simply do not have an adequate translation in the other language (e.g., in translation between English and Dutch the words `gezellig' and `evidence' do not seem to have a clear equivalent in the other language), and even if the same meaning is expressed there are many syntactic phenomena in natural language that seem to be problematic for a compositional translation.  For example: different ways of role expression (e.g., `I like (obj)' and `\textcyr{mne nravitsa (subj)}') or syntactic mismatches (e.g., `woonachtig zijn' and its translation `reside' \citep{landsbergen1989power}).  The grammar rules and basic units can thus not simply be taken from a monolingual grammar, as there is no guarantee that the rules and basic units will have a translation equivalent rule or basic unit in the other grammar. The grammars must be constructed for translation, such that they are `attuned' \citep{rosetta1994compositional}. \cite{rosetta1994compositional} showed that previous mentioned examples do not necessarily stand in the way of compositional translation, by manually constructing a grammar for translation from English to Dutch, covering many non-trivial translation phenomena. However, their grammar consist of separate semantic and syntactic formalisms, which makes their case more complicated than the more direct structure mapping we are interested in for this thesis.

In summary, even when the existence of compositional structures for languages and literalness of translation is assumed, there is no guarantee that the structures of the two languages can be linked in a consistent fashion. Suitable compositional structures for a language might thus depend on which language it should be translated in. The nature of this problem suggests that if compositional syntactic structures for languages  can be constructed, they should have a strong semantic motivation.

\subsection{In Practice}

Compositional structures of translation are representation-wise very similar to compositional structures of language: they are tree representations that describe how the sentence was compositionally translated. The difference lies in the notion of parts: in compositional language structures, the parts were sequences of words that were combined through syntactic relations, while in compositional translation structures, the parts are defined through translation equivalence. Consider, for instance, the sentence pair (I give my little brother a ball, Ik geef mijn kleine broertje een bal), in which the translation equivalence is made explicit through linking the `parts' that were used in the translation. Note that if tree pairs are constructed as such through compositionality, every source node tree has an equivalent target node tree, the two resulting trees are thus isomorphic.

\begin{figure}[!ht]
\begin{tikzpicture}


\node (I) at (1,0.06) {I};
\node (give) at (1.6,0.05) {give};
\node (my) at (2.3,0) {my};
\node (little) at (3.0,0.07) {little};
\node (brother) at (4.1,0.07) {brother};
\node (a) at (4.9,0.03) {a};
\node (ball) at (5.4,0.05) {ball};

\coordinate (toycar) at (5.6,0.7);
\coordinate (littlebrother) at (3.45,0.7);
\coordinate (mylittlebrother) at (3.2,1);
\coordinate (aball) at (5.2,0.8);
\coordinate (all) at (3.2,3);

\foreach \from/\to in {aball/ball, aball/a, littlebrother/little, littlebrother/brother, mylittlebrother/my, mylittlebrother/littlebrother, all/give, all/mylittlebrother, all/aball, all/I}
	\draw (\from) -- (\to);
	
\node (Ik) at (8,0.06) {Ik};
\node (geef) at (8.6,0) {geef};
\node (mijn) at (9.4,0) {mijn};
\node (kleine) at (10.3,0.04) {kleine};
\node (broertje) at (11.5,0.01) {broertje};
\node (een) at (12.5,0) {een};
\node (auto) at (13.15,0.05) {bal};

\coordinate (kleinebroertje) at (10.9,0.7) {};
\coordinate (mijnkleinebroertje) at (10.4,1.2);
\coordinate (eenauto) at (12.8,0.6);
\coordinate (alles) at (11,3);

\foreach \from/\to in {eenauto/auto, eenauto/een, kleinebroertje/kleine, kleinebroertje/broertje, mijnkleinebroertje/mijn, mijnkleinebroertje/kleinebroertje, alles/Ik, alles/geef, alles/mijnkleinebroertje, alles/eenauto}
	\draw (\from) -- (\to);	

\foreach \from/\to in {all/alles, mylittlebrother/mijnkleinebroertje, littlebrother/kleinebroertje, aball/eenauto}
	\draw[<->, bend left =25, very thick,blue] (\from) to (\to);

\foreach \from/\to in { broertje/brother,  mijn/my, Ik/I, auto/ball}
	\draw[<->, bend left =35, thick,blue] (\from) to (\to);

\foreach \from/\to in {een/a,  kleine/little, geef/give}
	\draw[<->, bend left =35, thick,green] (\from) to (\to);

\end{tikzpicture}

\caption{linked structures}\label{fig:transtrees}
\end{figure}

Unfortunately, the complexity of the given example is far below average. There is no translational divergence complicating the establishment of translation equivalence, the sentences have the same length, no reordering takes place,
and all translation equivalent sequences are contiguous. In fact, the target sentence is a word for word translation of the source sentence, which means that any source structure could be mapped to an isomorphic target structure through translation equivalence.

To establish isomorphism in harder cases, a phrasal approach is required. In Figure \ref{fig:phrasal}, a very simple example is given, of the translation of the three word phrase `a toy car' into the two word phrase `een speelgoedautootje'.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}

\node (a) at (0,0) {a};
\node (toy) at (0.5,0) {toy};
\node (car) at (1.1,0) {car};
\node (een) at (2.4,0) {een};
\node (auto) at (4.1,0) {speelgoedautootje};

\filldraw (0,0.2) circle (0.035);
\filldraw (2.45,0.2) circle (0.035);
\filldraw (4.0,0.2) circle (0.035);
\filldraw (0.9,0.5) circle (0.035);
\filldraw (0.6,1) circle (0.035);
\filldraw (3.2,1) circle (0.035);

\coordinate (toycar) at (0.9,0.5);
\coordinate (a_) at (0,0.2);
\coordinate (een_) at (2.45,0.2);
\coordinate (auto_) at (4.0,0.2);
\coordinate (all) at (0.6,1);
\coordinate (allnl) at (3.2,1);

\foreach \from/\to in {toycar/toy, toycar/car,all/toycar, all/a_, allnl/een_, allnl/auto_}
	\draw (\from) -- (\to);	

\foreach \from/\to in {auto/car,  auto/toy, a_/een_, toycar/auto_, all/allnl}
	\draw[<->, bend left =35, thick,blue] (\from) to (\to);

\draw[<->, bend left = 35, thick, green] (een) to (a);

\end{tikzpicture}
\caption{Phrasal translation equivalence}\label{fig:phrasal}
\end{figure}

From the previous two examples, it is not yet clear how such translation structures deal with reordering. It is important to realise that translation equivalence of two phrases does not imply that the order \textit{within} the phrases is identical. The phrases `una casa comoda' and `een comfortabel huis' are translation equivalent, even though the noun adjective order is not the same. Reordering by permuting the children in a parse tree is in practice quite powerful, although theoretically the fraction of reorderings that can be accounted for rapidly decreases with the length of the sentence \citep{satta2005some}.

Note that reordering amplifies the need of rules. Without them, it is impossible to always correctly guess the order of the leaf nodes of the target side tree.

\section{Combining Hypotheses}

We have discussed compositional structures for natural languages, we have discussed compositional translation structures, and we do now know what it means for them to coincide: the monolingual parts should have (contiguous) translation equivalent (contiguous) parts in the other sentence. There are often multiple compositional structures possible for source and target sides, depending on the intuitions of the creator of the structure. Furthermore, after establishing translation equivalence, there are often even more possible translation structures (... for the sentence pair in Figure \ref{fig:transtrees}), we have now thus arrived at the situation depicted in Figure  \ref{fig:comp2x}.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}

\coordinate (ss) at (1.5,0);
\node [below] at (ss) {compositional structures};

%\draw[] (0,0) -- node[below]{} (3,0);
\draw (0,0) -- (0.6,2) (3,0) -- (0.6,2);
%\draw (0,0) -- (0.9,2) (3,0) -- (0.9,2);
\draw (0,0) -- (1.2,2) (3,0) -- (1.2,2);
\draw (0,0) -- (1.5,2) (3,0) -- (1.5,2);
%\draw (0,0) -- (1.8,2) (3,0) -- (1.8,2);
\draw (0,0) -- (2.1,2) (3,0) -- (2.1,2);
\draw (0,0) -- (2.4,2) (3,0) -- (2.4,2);

\coordinate (ts) at (7.5,0);
\node [below] at (ts) {translation structures};

%\draw (6,0) -- (0.6,2) (9,0) -- (6.6,2);
\draw (6,0) -- (6.9,2) (9,0) -- (6.9,2);
\draw (6,0) -- (7.2,2) (9,0) -- (7.2,2);
%\draw (6,0) -- (7.5,2) (9,0) -- (7.5,2);
\draw (6,0) -- (7.8,2) (9,0) -- (7.8,2);
\draw (6,0) -- (8.1,2) (9,0) -- (8.1,2);
\draw (6,0) -- (8.4,2) (9,0) -- (8.4,2);

\coordinate (startarrow) at (3.1,1.2);
\coordinate (endarrow) at (5.9,1.2);
\node (t) at (4.5,1.7) [above]{consistency};

\draw[<->,bend left =35, thick] (startarrow) to (endarrow);

;\end{tikzpicture}
\caption{matching compositionality of translation and compositionality of natural language.}\label{fig:comp2x}
\end{figure}

We can find a set of structures according to which the sentence has been compositionally translated, we can find a set of structures that (possibly linguistically motivated) expresses the compositional structure of a sentence, but we do not know if it possibles to find consistency along these two sets. This topic is the main concern of this thesis.

\section{Empirically Studying Compositional Translation}

Thus far, we have discussed compositional translation on a practical yet unspecific level, as we have relied on intuition to determine both linguistic and translation equivalent parts (and structures). If compositional translation were to be used in an MT-model, however, means are needed to establish equivalences automatically and large scaled. The same holds, for empirical analyses of translation data (anders). In the current section, we will discuss these means, and the simplifications that are made in the process.

\subsection{Monolingual Compositionality}
\label{sec:depgram}

There are many monolingual grammar formalisms, few of which have parsers that can efficiently assign structures to large corpora of texts. The most common formalisms in MT are finite state machines, context free grammars, and dependency grammars. Although some cognitive researchers might disagree about the workings in the mind (ref rens?), the expressibility of finite state machines seems too weak to account for natural language. We will disregard them for the rest of this thesis. Constituency grammars have shown to be a quite suitable model for natural language, although language does not seem to be entirely context free \citep{shieber1987evidence}. Also, the labels of standard constituency grammars are based on the syntactic categories of words and phrases rather than being semantically motivated, which does seems suboptimal for translation. Although we will discuss some empirical work assessing the consistency of constituency grammars with translation data, in our own work we will solely focus on dependency grammars, which we will discuss elaborately in this subsection. Dependency grammars seem very suitable for the task at hand, as they are largely semantically motivated, and thus have potential to be consistent over language. We will shortly discuss the motivation and background of dependency grammars, give a formal definition and provide some clarifying examples.




\section{Foundations of Empirical Studies.}

We have now linked compositionality of language and compositionality of translation, and we have described means of generating compositional structures representing both of them. As we are dealing with real data produced by humans, we are not yet ready to start an empirical analysis, we have to make a few more assumptions. To appreciate empirical research, it is important to have a good picture of the simplifications that are made, and the influence they might have on the result. We will mention and discuss these assumptions, therewith describing the foundations of empirical research using translation data. At the end of this section, we will explicate a few concrete questions that can be addressed in the current framework.
%Given the size and nature of natural language, it is impossible to give an indisputable proof of the existence of such grammars, after all, how can one possibly show that a grammar covers all expressions that can be uttered by a human being. The theoretical question, however, evokes other questions of more practical nature, such as: (dit moet anders)
%
%\begin{itemize}
%\item How much of the structures generated by a grammar are respected by the translation data.
%\item If we assume linguistic structures on both source and target side are assumed, how far from bijective is the mapping needed to completely cover both trees.
%\item
%\end{itemize}


\subsection{Correctness of the Translation Data}

As empirical analyses are based on parallel corpora with text that are each others translation, it heavily relies on the correctness of these corpora. As these parallel texts were not designed as data for translation models, they might not be perfectly suitable for this purpose. When training MT models, infrequent mistakes in the data are generally not problematic, as they will receive a low probability. This is not the case with empirical analyses. There are three bottlenecks:

\myparagraph{1. Sentence level alignment.}
Aligning corpora on the sentence level is not as simple as it might seem. Texts are not always translated sentence by sentence. Short sentences may be merged or long ones broken up, and in some languages there are not even clear sentence delimiters \citep[p.55]{koehn2008statistical}. However, the techniques for sentence alignment are very good (ref??), it thus seems very reasonable to assume that the aligned sentences in the corpora do in fact have the same meaning.

\myparagraph{2. Correctness of translation. }
The texts are produced by humans, who sometimes make mistakes. To use the corpora, we have to assume that the aligned sentences are good translations of each other. Ref about quality??

\myparagraph{3. Translation is literal.}
One English sentence often has many translations in another language, as similar meanings can be expressed in multiple ways. For instance `jeg giver dig blomster' is a good Danish translation of `I give you flowers', but so is `jeg giver blomster til dig' (and the amount of rephrasing in this example is even rather minimal). Especially when one text is not a direct translation of the other text, but the two are, for instance, just separate reports of the same event, it might happen that sentences do have the same meaning, but are not very similar in form. In empirical analyses it is assumed that at least the vast majority of the translations in the corpora are rather literal.
 
\subsection{Word Alignments}

Also the previously discussed word-alignments are a bottleneck in empirical analysis of translation data. They are of crucial importance, as they are the mainstay for establishing translational correspondences, but are not always of high quality. Once again, MT-models do generally not suffer much from this fact, because the number of untrue alignment links is dwarfed by the number that is correct. For empirical research, false alignment links are quite problematic, as even one wrong link can have a huge effect on the space of possible translation trees. Empirical analyses therefore often use one of the few manually aligned corpora.

\subsection{Syntactic Parsers}

Not all models rely on linguistic information of the source and target languages, but if they do, automated parsers are required that provide this information. For English, high quality parsers are available, that make relatively little mistakes (refs?), although they may fail with rare phenomena. For other languages, this issue is often problematic.\\

Although statistical analysis of the corpora might provide more insight in the type of mistakes that are generally made, these types of mistakes will harm the outcome of an empirical analysis, making it more pessimistic than it would be if the data were perfect. It therefore seems sensible to see empirical analysis as an upper- or lowerbound (depending on the context), rather than an exact number.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OWN WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{Own work}

It seems that in all these analyses, still no sufficient answer is found to the most general empirical question (anders): is it possible to match up compositionality of translation and compositionality of language, such that an entire translation corpus can be covered. In this thesis, we will address this question, where we hope to implicitly also find an answer to the question: are predicate argument structures preserved over language. To do so, we will consider a broad set of translation structures: all maximally compositional tree structures in which the non-terminal nodes are contiguous parts, and the discontinuous units are generated as terminals. In other words: all source structures that can be mapped to a target structure through a bijective mapping of the non-terminal nodes \citep[this set was previously defined in][]{simaan2013hats}. We will devise an algorithm for generating these structures in an easily accessible format that allows for investigation of the structures.

In this thesis, we will focus on the consistency of the translation structures with dependency grammars, that we believe have, given their semantic nature, potential for being consistent across languages. We will, making the same assumptions with regard to the translation data earlier investors did, investigate how well the translation structures cohere with dependency parses, and study the main causes of deviation of the translation structures of conventional linguistic syntax.

The structure of this chapter is similar to the structure of the previous chapter. First, we will define the set of translation structures we are considering. We will give some examples to hopefully provide a more intuitive feeling for this set, and explain how we will obtain and represent these structures (Section \ref{sec:transstr}). The next section addresses our compositional system of language (Section \ref{sec:compstr}). As we have discussed dependency parses quite elaborately earlier on, this section is of a more practical nature than the previous one. In Section \ref{sec:comb}, we will discuss how we will combine the two compositionality systems, giving formal definitions of consistency and examples to illustrate and motivate our procedures.


\section{Translation Structures}
\label{sec:transstr}

The set of translation structures considered in this thesis is the set as described in \cite{simaan2013hats}. We will first give a formal description of this set of structures (\ref{subsec:hats}), and then try to give a more intuitive description that illustrates the power of the set (\ref{subsec:intuition}). In the last part of this section (\ref{subsec:representation}, we will discuss the representational system we will use to describe and assess these structures.


\section{Compositional Structures}

The compositional system for language we will use in this thesis is the dependency grammar, as was already indicated in Chapter \ref{ch:empirical}. In this chapter, we have argued for the suitability of dependency grammars as translation minded compositional system of language, gave some background information, and provided a formal description. A short recap: dependency structures describe the relation between words, instead of describing a hierarchical structure between phrases, aiming to capture the cognitive perception of sentences in the human brain. Dependency structures can be seen as predicate arguments structures of sentences, that are highly semantically motivated.

\subsection{Formally}

A rather formal description of dependency grammars was already given, we will use this subsection to sharpen notational affairs and define dependency parses as they are used in our experimental part. We will define a dependency tree as a set of relations, holding between the (positions of the) words of the sentence:

\begin{definition}[Dependency structure]
A dependency structure of a sentence $s = w_0\cdots w_n$ is a set of dependencies $D = \{ (i,j) |$ there is a dependency arrow from word $w_i$ to word $w_j \}$. 
\end{definition}

There are many conditions that can be imposed on $D$ \citep{de2006generating}, of which in this thesis the most common ones will used\begin{enumerate}
\item When seen as a relation, $D$ constitutes a single-headed a-cyclic graph in which the words in $s$ are the nodes. (tree-constraint)
\item When the words are placed in the original order, the branches of the dependendency tree do not cross. (projectivity)
\end{enumerate}

The span of a word according to a dependency tree is defined as follows:

\begin{definition}
If $T_d$ is a dependency tree for $s$, $w$ is a word in $s$ and $i$ and $j$ are the maximum and minimum positions, respectively, that can be reached from $w$ by following the directed dependency arrows. Then span($w$) = $[i,j]$. 
\end{definition}

\subsection{Generation and Representation}

To assign dependency structures to sentences, we used the Stanford Dependency Parser, that can be downloaded from their website, as well as used online  \citep{de2006generating}. The parser provides 5 variants of a typed dependency representation, of which the most basic one corresponds to the earlier imposed conditions on dependency structures. An example dependency parse as generated by the parser is depicted in Figure \ref{fig:deptree1}.

\begin{figure}[!h]\label{fig:deptree1}
\centering
\begin{dependency}[theme=simple]%[hide label]
\begin{deptext}[column sep=.5cm, row sep=.1ex]
%PRP\$ \& NN \& RB \&[.5cm] VBZ \& VBG \& NN \\
My \& dog \& also \& likes \& eating \& sausage \\
\end{deptext}
\deproot{4}{}
\depedge{2}{1}{poss}
\depedge{4}{2}{nsubj}
\depedge{4}{3}{xvmod}
\depedge{4}{5}{xcomp}
\depedge{5}{6}{dobj}
\end{dependency}
\caption{Stanford Dependency Tree}
\end{figure}

Say something about stanford dependency parsers not including punctuation


Formally, dependency grammars are not interpreted as a compositional grammar, as they do not postulate the existence of non-terminal syntactic categories, and therefore do not explicitly specify how a sentence was built up from its parts. However, dependency graphs do give rise to an hierarchical structure that specifies from which smaller parts the sentence was composed. For instance, the dependency graph depicted in Figure \ref{fig:deptree1} tells us that `likes' is the head word of the sentence, and that the sentence is composed of 4 parts: the head `likes', its modifier `also', its noun subject whose head is `dog' and the open clausal complement whose head is `eating'. The complement and subject are further divisible in `My' and `dog', and `eating' and `sausage', respectively. As the tree is projective, all parts are continuous. Also the graph in Figure \ref{fig:depgraph} prescribes an hierarchical structure: it is composed of the subject `I', the headword `know', and the phrase headed by `liet', that is in its turn built up from its head `liet', `dat', `hij' and the discontinous phrase `me winnen'. Such an hierarchical structure can not be captured by a phrase structure grammar.

\section{Combining Translation Structures and Dependency Parses}

To combine dependency parses and translation structures, means need to be find to quantify consistency and similarity between them. We have arrived at the situation depicted in Figure \ref{fig:depshats}.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}

\coordinate (ss) at (1.5,0);
\node [below] at (ss) {dependency parses};

\draw[->,bend right = 55,thick] (1.47,0) to (0,0);
\draw[->,bend left = 55, thick] (1.53,0) to (3,0);
\draw[->,bend left = 55, thick] (0.0,0) to (1.2,0);
\draw[->,bend right = 55,thick] (1.12,0) to (0.5,0);
\draw[->,bend left = 55, thick] (1.53,0) to (2.0,0);
\draw[->,bend right = 55,thick] (2.9,0) to (2.3,0);

\coordinate (ts) at (7.5,0);
\node [below] at (ts) {compositional translation structures};

%\draw (6,0) -- (0.6,2) (9,0) -- (6.6,2);
\draw (6,0) -- (6.9,2) (9,0) -- (6.9,2);
\draw (6,0) -- (7.2,2) (9,0) -- (7.2,2);
%\draw (6,0) -- (7.5,2) (9,0) -- (7.5,2);
\draw (6,0) -- (7.8,2) (9,0) -- (7.8,2);
\draw (6,0) -- (8.1,2) (9,0) -- (8.1,2);
\draw (6,0) -- (8.4,2) (9,0) -- (8.4,2);

\coordinate (startarrow) at (3.1,0.7);
\coordinate (endarrow) at (5.9,1.2);
\node (t) at (4.4,1.5) [above]{consistency?};

\draw[<->,bend left =35, thick] (startarrow) to (endarrow);

\end{tikzpicture}
\caption{New situations: finding consistency between dependency parses and compositional translation structures}\label{fig:depshats}
\end{figure}

\noindent In this section, we will define coherence between dependency parses and compositional translation structures. In particular, we will define two methods to determine if a dependency parse is respected by a compositional translation tree,  and a recursive procedure for assigning a similarity score to compositional translation trees accordingly. 

\subsection{Direct Similarity}

Intuitively, a compositional translation tree is consistent with a dependency tree if they prescribe the same parts. An obvious similarity choice would thus be F-score (ref?), according to which the output of syntactic constituency parsers is evaluated. However, as we are comparing with dependency relations between words, it seems just as important that the parts are correctly combined, which is accounted for in our new definition of consistency. 

\begin{metric}[Consistency between Dependency relation and Alignment Tree]\label{met:depHAT}
Let $s = w_1 w_2 \dots w_n$ be a sentence, and $D = \{ (i,j) |$ there is a dependency arrow from word $w_i$ to word $w_j \}$ a set of dependencies describing a dependency tree for $s$. Let span($j$) be the range $[m,n]$ in which $m$ and $n$ are the maximum and minimum position that can be reached from $w_j$ by following the directed dependency arrows, respectively. A dependency relation $(i,j)$ is said to be respected by an alignment tree $T$ over $s$ if and only if there is a node in $T$ of which both $[i,i]$ and span($j$) are children.
\end{metric}

\noindent This consistency definition expresses the intuition that a predicate and its arguments are all generated one syntactic rule, and should thus be children of the same node. A dependency relation is respected by a compositional translation tree if the head and the entire phrase the dependent is heading are siblings in the tree.

\subsection{Deeper Similarity?}

The type of consistency defined in Definition \ref{def:depHAT} is very direct, and might assign many trees that seem perfectly in line with dependency parses a rather low score, due to different types of recursivity. Dependency structures are created by linguists, and although they certainly have a generality too them, they are not constructed to be \textit{maximally} recursive, as the set of used translation structures. We will give two examples to illustrate this.

Firstly, consider the sentence `I give you flowers', which contains one predicate (`give') with three arguments (the subject `I', the object `flowers' and the indirect object `you'). Its Dutch translation `Ik geef jou bloemen' has exactly the same predicate argument structure \textit{and} word-order, whereby the branching factor in any of its HATs will not exceed two. However, a maximum score can only be obtained by a tree in which `I', `give', `you', and `flowers' are siblings, whose mother will have a branching factor of (at least) four. Even though the translation of this sentence seems perfectly compositional, no HAT will thus obtain the maximum score, because the dependency structure is not minimally branching.

A second example, that is more related to translational divergence, arises when two arguments are translated into one (which happens, e.g., when arguments are translated as pre- or suffixes, when verbs do not require a subject or when spaces are emitted). Consider for instance the sentence 'Can you give me the salt' and its Italian translation 'puoi passarmi il sale'. Once again, the predicate-argument structure of the sentence is well preserved. However, the dependency parse prescribes that `the salt', `can' and `you' should be siblings of `give', which will be the case in none of the HATs over $\{\{0\},\{0\},\{1\},\{1\},\{2\},\{3\}\}$, as `give' and `me' are together translated into `passarmi', and `can you' into `puoi'.

To account for this discrepancy between maximally recursive alignment structures and dependency parses, we allow a predicate to combine with its arguments 'one-by-one', which would allow both given examples to get a maximal score. A dependency relation is thus respected by a compositional translation structure if the phrase headed by the dependent is siblings with the head itself, or the head plus arguments the head earlier combined with, which is defined in Definition \ref{met:depHAT2}.

\begin{metric}[Consistency2]\label{met:depHAT2}
Let $s = w_1 w_2 \dots w_n$ be a sentence, and $D = \{ (i,j) |$ there is a dependency arrow from word $w_i$ to word $w_j \}$ a set of dependencies describing a dependency tree for $s$. Let span($j$) be the range $[m,n]$ in which $m$ and $n$ are the maximum and minimum position that can be reached from $w_j$ by following the directed dependency arrows, respectively. Let $(i,j)$ be a dependency relation in $D$, and let $l_1,\ldots,l_n$ and $r_1,\ldots r_k$ be the left and right dependents of $i$, respectively, for which holds that $r_k < j$ or $l_1 > j$. A dependency relation $(i,j)$ is said to be respected by an alignment tree $T$ over $s$ if and only if one of the following three conditions is true: \begin{enumerate}
\item There is a node in $T$ f which both $[i,i]$ and span($j$) are children.
\item $\exists x$  and a node in $T$ of which span($l_x\ldots l_n~i~\ldots r_1 r_k$) and span($j$) are both children.
\item $\exists x$  and a node in $T$ of which span($l_1\ldots l_n~i~r_1\ldots r_x$) and span($j$) are both children.
\end{enumerate} 
\end{metric}


\subsection{Scoring Trees}

The (not yet normalised) score of a compositional translation tree can now be recursively defined as follows:

\begin{definition}[Scoring]
Let $s = w_1 w_2 \dots w_n$ be a sentence, and $D = \{ (i,j) |$ there is a dependency arrow from word $w_i$ to word $w_j \}$ a set of dependencies describing a dependency tree for $s$. Let $d_{i,j}$ be the set of relations that could make $(i,j)$ true in $T$, let $D'$ = $\{d_{i,j}| (i,j)\in D\}$. Let $H$ be an alignment tree for $s$. The (unnormalised) score of $H$ with $D$ is now defined as the score of its highest node $N$:

$$
E(N_a,D) = \sum_{c\in C_{N_a}} E(c,D)+ \sum_{c_1\in C_{N_a}} \sum_{c_2\in C_{N_a}} B(c_1,c_2)
$$

\noindent With base case $E(N,D) = 0$, $B(c_1,c_2) = 1$ iff  $(c_1,c_2)\in D'$, and $C_N$ the set of child nodes of $N$.

The score can be normalised by dividing by $|D|$.
\end{definition}

An example is given in the next subsection.

\subsection{Example}

We will provide an example for scoring the similarity between a compositional translation structure and a dependency parse according to direct similarity. Consider once again the dependency depicted in Figure \ref{fig:deptree1}. The set of dependencies $D$ is:

$$ \{ (0,4), (2,1), (4,2), (4,3), (4,5), (5,6) \}.$$


\noindent As direct similarity is used as consistency measure, $d_{i,j}$ = $[i,i], span(j)]$, thus we have:

$$D' = \{([1,1],[2,2]), ([4,4],[1,2]), ([4,4], [3,3]), ([4,4],[5,6]), ([5,5],[6,6])\}$$

\noindent The following tree (which is not maximally recursive) would receive the optimal score of 5/5, as all the 5 dependencies are present in the tree:\\

\Tree [.{[}1,6] [.{[}1,2] [.{[}1,1] my ] [.{[}2,2] dog ] ] [.{[}3,3] also ] [.{[}4,4] likes ] [.{[}5,6] [.{[}5,5] eating ] [.{[}6,6] sausage ] ]  ]

\noindent While the tree (also not maximally recursive)

\Tree [.{[}1,6] [.{[}1,2] [.{[}1,1] my ] [.{[}2,2] dog ] ] [ [.{[}3,3] also ] ] [.{[}4,6] [.{[}4,4] likes ] [.{[}5,6] [.{[}5,5] eating ] [.{[}6,6] sausage ] ] ] ] 

\noindent receives a slightly lower score of 3/5 as we miss dependencies (likes, also) and (likes, dog).



\chapter{Empirical Analysis}

In the previous chapters, we have discussed the theoretical foundations of our work, as well as the techniques and tools that we have used in our experiments. In this chapter, we will discuss the empirical part of this thesis, which will include describing the results of our tests, and comparing them with the results of others.  We will start by 


We have ....
In this chapter, we will discuss the results of the automatic analysis of the translation data, and manually analyse these. (?)
This chapter is structured as follows. In Section \ref{sec:data}, we will start with a description of the data we have used for our analyses .. blablabla In Section \ref{sec:results1}, we will summarise the results of our automatic experiments. In \ref{sec:disc.}, we will discuss these results, and present a manual analysis of....
 discuss our results on the automatically aligned corpora

We will start with a description of our data, and the exact tools used to blablabla. In section ... blablabla





\section{Data}
\label{sec:data}

%Maybe give some other statistics on the data?
For our experiments we have used several datasets, both manually and automatically aligned. This section contains a description of our datasets and the tools used to align and parse them, that can be used for replication of the experiments.

\subsection{Automatically Aligned Corpora}

We had four automatically aligned corpora available, from each of which we drew 200,000 sentences for our experiments. 

The iterations that GIZA performed are:

m1: 4
hmm model: 3
m3: 3
m4: 3

In general the guidelines for building a baseline for the WMT workshops were followed. See:
http://www.statmt.org/wmt07/baseline.html

The corpora were tokenised using the relevant moses script and lowercased before GIZA was run.



We ran our tests on several corpora, that were either aligned automatically or manually.  We will now give a short description of the data used, a summary can be found in \ref{tab:datasets}.

\subsubsection{Automatically Aligned Corpora}
The larger, automatically aligned, corpora for English-French, English-Dutch and English-German consisted of 200.000 sentences from the Europarl corpus (ref?), while the English-Chinese corpus used consisted of 200.000 sentences from .... . The corpora from which the sentences were drawn were all automatically aligned with the Giza++ tool \citep{koehn2007moses} designed by IBM, using the previously described `grow-diag-and-final'-heuristic, with 4 iterations on model 1, 3 on the hmm model, model 3 and model 4. The corpora were tokenised and lowercased before GIZA was run.


\subsubsection{Manually Aligned Corpora}
To investigate the influence of faulty alignment links on our result, we also ran our tests on manually aligned data, whose alignment links are more reliable and that are thus more suitable for empirical analysis. As far as the author knows, no manually aligned corpora for translation from English to Dutch are available, we did find two manually aligned corpora for translation from English to French, and one for translation from English to German. In addition, we analysed  2 new language pairs, of which manually aligned corpora were available: English-Portuguese and English-Spanish. The data were manually aligned by \cite{graca2008building} (en-fr, en-sp and en-pt), \cite{pado2006optimal} (en-de) and \cite{och2000improved} (en-fr).

\begin{table}
\begin{tabular}{llllll}
Language & Corpus & & Section & Aligned\\
\hline
English - Dutch & Europarl & & automatic \\
English - French & Europarl & automatic \\
English - German & Europarl & automatic \\
English - Chinese & & & automatic \\
English- French & Hansard & & manual\\
\end{tabular}
\caption{Datasets}\label{tab:datasets}
\end{table}

\subsection{Dependency Relations}

The translation direction considered was always from English to foreign language, such that just one parser was required. The English side of the corpus was parsed with the Stanford dependency parser \citep{de2008stanford} using newlines as delimitation, as to respect the sentence alignment of the corpora, to obtain basic typed dependencies of the sentences. Note that the dependency parser used does not take into account punctuation, which sometimes complicates the analysis.


\section{Replications}

We have

\section{Experiments}

All experiments were conducted in a similar fashion: we generated a grammar for every sentence pair in the corpus, and found the best tree and its score through a CKY chart parsing algorithm with the resulting grammar, making use of an external parser \citep{birch2010lrscore}. Documentation of the implementation used can be found in Appendix \ref{appendix:impl}, and the code, including several demo's is available on www.github.nl/dieuwkehupkes. In this section we will discuss the results of our experiments, and some difficulties that arose in obtaining them.


\subsection{Direct Similarity}

We measured the consistency of the corpus through similarity metric \ref{met:depHAT}. The results scores of the four automatically aligned datasets can be found in Table \ref{tab:scores1}.

\begin{table}[!h]
\centering
\begin{tabular}{l|ccc}
& \multicolumn{3}{c}{Scores}\\
Language pair & |s| < 10 & |s| < 20 & |s| < 40\\
\hline
English-Dutch & 0.47 & 0.42 & 0.40 \\
English-French & 0.46 & 0.42 & 041 \\
English-German & 0.44 & 0.41 & 0.38 \\
English-Chinese & 0.59 & 0.48 & 0.42\\
\end{tabular}
\caption{Results Experiment 1, automatic alignments}\label{tab:scores1}
\end{table}

As expected, the scores are rather low, due to discrepancy between the relatively flat dependency parses and maximally compositional translation structures. The scores for the manually aligned corpora, that are not much higher, can be found in Table \ref{tab:scores2}. Unfortunately, the manually aligned datasets are not identical to the 

\begin{table}[!h]
\centering
\begin{tabular}{l|ccc}
& \multicolumn{3}{c}{Scores}\\
Language pair & |s| < 10 & |s| < 20 & |s| < 40\\
\hline
English-German (Pado) & 0.43 & 0.42 & 0.41 \\
English-French (Hansard) & 0.63 & 0.54 & 0.51 \\
English-French (LREC) & 0.49 & 0.47 & 0.47 \\
English-Portuguese (LREC) & 0.47 & 0.45 & 0.45 \\
English- Spanish (LREC) & 0.51 & 0.48 & 0.48\\
\end{tabular}
\caption{Results Experiment 1, Manual Alignments}\label{tab:scores2}
\end{table}

\subsection{Completer Similarity ??}

The results of our second experiment, in which we allows arguments of dependent heads to combine in different stages, are of more interest, as they provide a more thorough picture of the similarity between the two types of compositionality. As expected, the scores are much higher, as can be seen in Table \ref{tab:scores3}.

\begin{table}[!h]
\centering
\begin{tabular}{l|ccc}
& \multicolumn{3}{c}{Scores}\\
Language pair & |s| < 10 & |s| < 20 & |s| < 40\\
\hline
English-Dutch & 0.79 & 0.74 & 0.71 \\
English-French & 0.80 & 0.77 & 0.76\\
English-German & 0.75 & 0.71 & 0.68 \\
English-Chinese & 0.76 & 0.67 & 0.62\\
\end{tabular}
\caption{Results Experiment 2, automatic alignments}\label{tab:scores3}
\end{table}

blabla

\begin{table}[!h]
\centering
\begin{tabular}{l|ccc}
& \multicolumn{3}{c}{Scores}\\
Language pair & |s| < 10 & |s| < 20 & |s| < 40\\
\hline
English-German (Pado) & 0.82 & 0.80 & 0.77 \\
English-French (Hansard) & 0.85 & 0.80 & 0.78 \\
English-French (LREC) & 0.78 & 0.82 & 0.82 \\
English-Portuguese (LREC) & 0.75 & 0.76 & 0.76 \\
English- Spanish (LREC) & 0.79 & 0.80 & 0.80\\
\end{tabular}
\caption{Results Experiment 2, Manual Alignments}\label{tab:scores2}
\end{table}


\appendix
\chapter{Implementation}
\label{appendix:impl}

\chapter{Manual Analysis}
\label{appendix:analysis}



\bibliography{thesisDH}
\end{document}