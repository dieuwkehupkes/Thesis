\documentclass{report}
\input{commands.tex}

\begin{document}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\input{introduction.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BACKGROUND MT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{Ch2.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BACKGROUND EMPIRICAL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
%\input{Ch3.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5 
% OWN WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
%\input{Ch4.tex} 









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OWN WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{Own work}

It seems that in all these analyses, still no sufficient answer is found to the most general empirical question (anders): is it possible to match up compositionality of translation and compositionality of language, such that an entire translation corpus can be covered. In this thesis, we will address this question, where we hope to implicitly also find an answer to the question: are predicate argument structures preserved over language. To do so, we will consider a broad set of translation structures: all maximally compositional tree structures in which the non-terminal nodes are contiguous parts, and the discontinuous units are generated as terminals. In other words: all source structures that can be mapped to a target structure through a bijective mapping of the non-terminal nodes \citep[this set was previously defined in][]{simaan2013hats}. We will devise an algorithm for generating these structures in an easily accessible format that allows for investigation of the structures.

In this thesis, we will focus on the consistency of the translation structures with dependency grammars, that we believe have, given their semantic nature, potential for being consistent across languages. We will, making the same assumptions with regard to the translation data earlier investors did, investigate how well the translation structures cohere with dependency parses, and study the main causes of deviation of the translation structures of conventional linguistic syntax.

The structure of this chapter is similar to the structure of the previous chapter. First, we will define the set of translation structures we are considering. We will give some examples to hopefully provide a more intuitive feeling for this set, and explain how we will obtain and represent these structures (Section \ref{sec:transstr}). The next section addresses our compositional system of language (Section \ref{sec:compstr}). As we have discussed dependency parses quite elaborately earlier on, this section is of a more practical nature than the previous one. In Section \ref{sec:comb}, we will discuss how we will combine the two compositionality systems, giving formal definitions of consistency and examples to illustrate and motivate our procedures.


\section{Translation Structures}
\label{sec:transstr}

The set of translation structures considered in this thesis is the set as described in \cite{simaan2013hats}. We will first give a formal description of this set of structures (\ref{subsec:hats}), and then try to give a more intuitive description that illustrates the power of the set (\ref{subsec:intuition}). In the last part of this section (\ref{subsec:representation}, we will discuss the representational system we will use to describe and assess these structures.


\section{Compositional Structures}

The compositional system for language we will use in this thesis is the dependency grammar, as was already indicated in Chapter \ref{ch:empirical}. In this chapter, we have argued for the suitability of dependency grammars as translation minded compositional system of language, gave some background information, and provided a formal description. A short recap: dependency structures describe the relation between words, instead of describing a hierarchical structure between phrases, aiming to capture the cognitive perception of sentences in the human brain. Dependency structures can be seen as predicate arguments structures of sentences, that are highly semantically motivated.

\subsection{Formally}

A rather formal description of dependency grammars was already given, we will use this subsection to sharpen notational affairs and define dependency parses as they are used in our experimental part. We will define a dependency tree as a set of relations, holding between the (positions of the) words of the sentence:

\begin{definition}[Dependency structure]
A dependency structure of a sentence $s = w_0\cdots w_n$ is a set of dependencies $D = \{ (i,j) |$ there is a dependency arrow from word $w_i$ to word $w_j \}$. 
\end{definition}

There are many conditions that can be imposed on $D$ \citep{de2006generating}, of which in this thesis the most common ones will used\begin{enumerate}
\item When seen as a relation, $D$ constitutes a single-headed a-cyclic graph in which the words in $s$ are the nodes. (tree-constraint)
\item When the words are placed in the original order, the branches of the dependendency tree do not cross. (projectivity)
\end{enumerate}

The span of a word according to a dependency tree is defined as follows:

\begin{definition}
If $T_d$ is a dependency tree for $s$, $w$ is a word in $s$ and $i$ and $j$ are the maximum and minimum positions, respectively, that can be reached from $w$ by following the directed dependency arrows. Then span($w$) = $[i,j]$. 
\end{definition}

\subsection{Generation and Representation}

To assign dependency structures to sentences, we used the Stanford Dependency Parser, that can be downloaded from their website, as well as used online  \citep{de2006generating}. The parser provides 5 variants of a typed dependency representation, of which the most basic one corresponds to the earlier imposed conditions on dependency structures. An example dependency parse as generated by the parser is depicted in Figure \ref{fig:deptree1}.

\begin{figure}[!h]\label{fig:deptree1}
\centering
\begin{dependency}[theme=simple]%[hide label]
\begin{deptext}[column sep=.5cm, row sep=.1ex]
%PRP\$ \& NN \& RB \&[.5cm] VBZ \& VBG \& NN \\
My \& dog \& also \& likes \& eating \& sausage \\
\end{deptext}
\deproot{4}{}
\depedge{2}{1}{poss}
\depedge{4}{2}{nsubj}
\depedge{4}{3}{xvmod}
\depedge{4}{5}{xcomp}
\depedge{5}{6}{dobj}
\end{dependency}
\caption{Stanford Dependency Tree}
\end{figure}

Say something about stanford dependency parsers not including punctuation


Formally, dependency grammars are not interpreted as a compositional grammar, as they do not postulate the existence of non-terminal syntactic categories, and therefore do not explicitly specify how a sentence was built up from its parts. However, dependency graphs do give rise to an hierarchical structure that specifies from which smaller parts the sentence was composed. For instance, the dependency graph depicted in Figure \ref{fig:deptree1} tells us that `likes' is the head word of the sentence, and that the sentence is composed of 4 parts: the head `likes', its modifier `also', its noun subject whose head is `dog' and the open clausal complement whose head is `eating'. The complement and subject are further divisible in `My' and `dog', and `eating' and `sausage', respectively. As the tree is projective, all parts are continuous. Also the graph in Figure \ref{fig:depgraph} prescribes an hierarchical structure: it is composed of the subject `I', the headword `know', and the phrase headed by `liet', that is in its turn built up from its head `liet', `dat', `hij' and the discontinous phrase `me winnen'. Such an hierarchical structure can not be captured by a phrase structure grammar.

\section{Combining Translation Structures and Dependency Parses}

To combine dependency parses and translation structures, means need to be find to quantify consistency and similarity between them. We have arrived at the situation depicted in Figure \ref{fig:depshats}.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}

\coordinate (ss) at (1.5,0);
\node [below] at (ss) {dependency parses};

\draw[->,bend right = 55,thick] (1.47,0) to (0,0);
\draw[->,bend left = 55, thick] (1.53,0) to (3,0);
\draw[->,bend left = 55, thick] (0.0,0) to (1.2,0);
\draw[->,bend right = 55,thick] (1.12,0) to (0.5,0);
\draw[->,bend left = 55, thick] (1.53,0) to (2.0,0);
\draw[->,bend right = 55,thick] (2.9,0) to (2.3,0);

\coordinate (ts) at (7.5,0);
\node [below] at (ts) {compositional translation structures};

%\draw (6,0) -- (0.6,2) (9,0) -- (6.6,2);
\draw (6,0) -- (6.9,2) (9,0) -- (6.9,2);
\draw (6,0) -- (7.2,2) (9,0) -- (7.2,2);
%\draw (6,0) -- (7.5,2) (9,0) -- (7.5,2);
\draw (6,0) -- (7.8,2) (9,0) -- (7.8,2);
\draw (6,0) -- (8.1,2) (9,0) -- (8.1,2);
\draw (6,0) -- (8.4,2) (9,0) -- (8.4,2);

\coordinate (startarrow) at (3.1,0.7);
\coordinate (endarrow) at (5.9,1.2);
\node (t) at (4.4,1.5) [above]{consistency?};

\draw[<->,bend left =35, thick] (startarrow) to (endarrow);

\end{tikzpicture}
\caption{New situations: finding consistency between dependency parses and compositional translation structures}\label{fig:depshats}
\end{figure}

\noindent In this section, we will define coherence between dependency parses and compositional translation structures. In particular, we will define two methods to determine if a dependency parse is respected by a compositional translation tree,  and a recursive procedure for assigning a similarity score to compositional translation trees accordingly. 

\subsection{Direct Similarity}

Intuitively, a compositional translation tree is consistent with a dependency tree if they prescribe the same parts. An obvious similarity choice would thus be F-score (ref?), according to which the output of syntactic constituency parsers is evaluated. However, as we are comparing with dependency relations between words, it seems just as important that the parts are correctly combined, which is accounted for in our new definition of consistency. 

\begin{metric}[Consistency between Dependency relation and Alignment Tree]\label{met:depHAT}
Let $s = w_1 w_2 \dots w_n$ be a sentence, and $D = \{ (i,j) |$ there is a dependency arrow from word $w_i$ to word $w_j \}$ a set of dependencies describing a dependency tree for $s$. Let span($j$) be the range $[m,n]$ in which $m$ and $n$ are the maximum and minimum position that can be reached from $w_j$ by following the directed dependency arrows, respectively. A dependency relation $(i,j)$ is said to be respected by an alignment tree $T$ over $s$ if and only if there is a node in $T$ of which both $[i,i]$ and span($j$) are children.
\end{metric}

\noindent This consistency definition expresses the intuition that a predicate and its arguments are all generated one syntactic rule, and should thus be children of the same node. A dependency relation is respected by a compositional translation tree if the head and the entire phrase the dependent is heading are siblings in the tree.

\subsection{Deeper Similarity?}

The type of consistency defined in Definition \ref{def:depHAT} is very direct, and might assign many trees that seem perfectly in line with dependency parses a rather low score, due to different types of recursivity. Dependency structures are created by linguists, and although they certainly have a generality too them, they are not constructed to be \textit{maximally} recursive, as the set of used translation structures. We will give two examples to illustrate this.

Firstly, consider the sentence `I give you flowers', which contains one predicate (`give') with three arguments (the subject `I', the object `flowers' and the indirect object `you'). Its Dutch translation `Ik geef jou bloemen' has exactly the same predicate argument structure \textit{and} word-order, whereby the branching factor in any of its HATs will not exceed two. However, a maximum score can only be obtained by a tree in which `I', `give', `you', and `flowers' are siblings, whose mother will have a branching factor of (at least) four. Even though the translation of this sentence seems perfectly compositional, no HAT will thus obtain the maximum score, because the dependency structure is not minimally branching.

A second example, that is more related to translational divergence, arises when two arguments are translated into one (which happens, e.g., when arguments are translated as pre- or suffixes, when verbs do not require a subject or when spaces are emitted). Consider for instance the sentence 'Can you give me the salt' and its Italian translation 'puoi passarmi il sale'. Once again, the predicate-argument structure of the sentence is well preserved. However, the dependency parse prescribes that `the salt', `can' and `you' should be siblings of `give', which will be the case in none of the HATs over $\{\{0\},\{0\},\{1\},\{1\},\{2\},\{3\}\}$, as `give' and `me' are together translated into `passarmi', and `can you' into `puoi'.

To account for this discrepancy between maximally recursive alignment structures and dependency parses, we allow a predicate to combine with its arguments 'one-by-one', which would allow both given examples to get a maximal score. A dependency relation is thus respected by a compositional translation structure if the phrase headed by the dependent is siblings with the head itself, or the head plus arguments the head earlier combined with, which is defined in Definition \ref{met:depHAT2}.

\begin{metric}[Consistency2]\label{met:depHAT2}
Let $s = w_1 w_2 \dots w_n$ be a sentence, and $D = \{ (i,j) |$ there is a dependency arrow from word $w_i$ to word $w_j \}$ a set of dependencies describing a dependency tree for $s$. Let span($j$) be the range $[m,n]$ in which $m$ and $n$ are the maximum and minimum position that can be reached from $w_j$ by following the directed dependency arrows, respectively. Let $(i,j)$ be a dependency relation in $D$, and let $l_1,\ldots,l_n$ and $r_1,\ldots r_k$ be the left and right dependents of $i$, respectively, for which holds that $r_k < j$ or $l_1 > j$. A dependency relation $(i,j)$ is said to be respected by an alignment tree $T$ over $s$ if and only if one of the following three conditions is true: \begin{enumerate}
\item There is a node in $T$ f which both $[i,i]$ and span($j$) are children.
\item $\exists x$  and a node in $T$ of which span($l_x\ldots l_n~i~\ldots r_1 r_k$) and span($j$) are both children.
\item $\exists x$  and a node in $T$ of which span($l_1\ldots l_n~i~r_1\ldots r_x$) and span($j$) are both children.
\end{enumerate} 
\end{metric}


\subsection{Scoring Trees}

The (not yet normalised) score of a compositional translation tree can now be recursively defined as follows:

\begin{definition}[Scoring]
Let $s = w_1 w_2 \dots w_n$ be a sentence, and $D = \{ (i,j) |$ there is a dependency arrow from word $w_i$ to word $w_j \}$ a set of dependencies describing a dependency tree for $s$. Let $d_{i,j}$ be the set of relations that could make $(i,j)$ true in $T$, let $D'$ = $\{d_{i,j}| (i,j)\in D\}$. Let $H$ be an alignment tree for $s$. The (unnormalised) score of $H$ with $D$ is now defined as the score of its highest node $N$:

$$
E(N_a,D) = \sum_{c\in C_{N_a}} E(c,D)+ \sum_{c_1\in C_{N_a}} \sum_{c_2\in C_{N_a}} B(c_1,c_2)
$$

\noindent With base case $E(N,D) = 0$, $B(c_1,c_2) = 1$ iff  $(c_1,c_2)\in D'$, and $C_N$ the set of child nodes of $N$.

The score can be normalised by dividing by $|D|$.
\end{definition}

An example is given in the next subsection.

\subsection{Example}

We will provide an example for scoring the similarity between a compositional translation structure and a dependency parse according to direct similarity. Consider once again the dependency depicted in Figure \ref{fig:deptree1}. The set of dependencies $D$ is:

$$ \{ (0,4), (2,1), (4,2), (4,3), (4,5), (5,6) \}.$$


\noindent As direct similarity is used as consistency measure, $d_{i,j}$ = $[i,i], span(j)]$, thus we have:

$$D' = \{([1,1],[2,2]), ([4,4],[1,2]), ([4,4], [3,3]), ([4,4],[5,6]), ([5,5],[6,6])\}$$

\noindent The following tree (which is not maximally recursive) would receive the optimal score of 5/5, as all the 5 dependencies are present in the tree:\\

\Tree [.{[}1,6] [.{[}1,2] [.{[}1,1] my ] [.{[}2,2] dog ] ] [.{[}3,3] also ] [.{[}4,4] likes ] [.{[}5,6] [.{[}5,5] eating ] [.{[}6,6] sausage ] ]  ]

\noindent While the tree (also not maximally recursive)

\Tree [.{[}1,6] [.{[}1,2] [.{[}1,1] my ] [.{[}2,2] dog ] ] [ [.{[}3,3] also ] ] [.{[}4,6] [.{[}4,4] likes ] [.{[}5,6] [.{[}5,5] eating ] [.{[}6,6] sausage ] ] ] ] 

\noindent receives a slightly lower score of 3/5 as we miss dependencies (likes, also) and (likes, dog).



\chapter{Empirical Analysis}

In the previous chapters, we have discussed the theoretical foundations of our work, as well as the techniques and tools that we have used in our experiments. In this chapter, we will discuss the empirical part of this thesis, which will include describing the results of our tests, and comparing them with the results of others.  We will start by 


We have ....
In this chapter, we will discuss the results of the automatic analysis of the translation data, and manually analyse these. (?)
This chapter is structured as follows. In Section \ref{sec:data}, we will start with a description of the data we have used for our analyses .. blablabla In Section \ref{sec:results1}, we will summarise the results of our automatic experiments. In \ref{sec:disc.}, we will discuss these results, and present a manual analysis of....
 discuss our results on the automatically aligned corpora

We will start with a description of our data, and the exact tools used to blablabla. In section ... blablabla





\section{Data}
\label{sec:data}

%Maybe give some other statistics on the data?
For our experiments we have used several datasets, both manually and automatically aligned. This section contains a description of our datasets and the tools used to align and parse them, that can be used for replication of the experiments.

\subsection{Automatically Aligned Corpora}

We had four automatically aligned corpora available, from each of which we drew 200,000 sentences for our experiments. 

The iterations that GIZA performed are:

m1: 4
hmm model: 3
m3: 3
m4: 3

In general the guidelines for building a baseline for the WMT workshops were followed. See:
http://www.statmt.org/wmt07/baseline.html

The corpora were tokenised using the relevant moses script and lowercased before GIZA was run.



We ran our tests on several corpora, that were either aligned automatically or manually.  We will now give a short description of the data used, a summary can be found in \ref{tab:datasets}.

\subsubsection{Automatically Aligned Corpora}
The larger, automatically aligned, corpora for English-French, English-Dutch and English-German consisted of 200.000 sentences from the Europarl corpus (ref?), while the English-Chinese corpus used consisted of 200.000 sentences from .... . The corpora from which the sentences were drawn were all automatically aligned with the Giza++ tool \citep{koehn2007moses} designed by IBM, using the previously described `grow-diag-and-final'-heuristic, with 4 iterations on model 1, 3 on the hmm model, model 3 and model 4. The corpora were tokenised and lowercased before GIZA was run.


\subsubsection{Manually Aligned Corpora}
To investigate the influence of faulty alignment links on our result, we also ran our tests on manually aligned data, whose alignment links are more reliable and that are thus more suitable for empirical analysis. As far as the author knows, no manually aligned corpora for translation from English to Dutch are available, we did find two manually aligned corpora for translation from English to French, and one for translation from English to German. In addition, we analysed  2 new language pairs, of which manually aligned corpora were available: English-Portuguese and English-Spanish. The data were manually aligned by \cite{graca2008building} (en-fr, en-sp and en-pt), \cite{pado2006optimal} (en-de) and \cite{och2000improved} (en-fr).

\begin{table}
\begin{tabular}{llllll}
Language & Corpus & & Section & Aligned\\
\hline
English - Dutch & Europarl & & automatic \\
English - French & Europarl & automatic \\
English - German & Europarl & automatic \\
English - Chinese & & & automatic \\
English- French & Hansard & & manual\\
\end{tabular}
\caption{Datasets}\label{tab:datasets}
\end{table}

\subsection{Dependency Relations}

The translation direction considered was always from English to foreign language, such that just one parser was required. The English side of the corpus was parsed with the Stanford dependency parser \citep{de2008stanford} using newlines as delimitation, as to respect the sentence alignment of the corpora, to obtain basic typed dependencies of the sentences. Note that the dependency parser used does not take into account punctuation, which sometimes complicates the analysis.


\section{Replications}

We have

\section{Experiments}

All experiments were conducted in a similar fashion: we generated a grammar for every sentence pair in the corpus, and found the best tree and its score through a CKY chart parsing algorithm with the resulting grammar, making use of an external parser \citep{birch2010lrscore}. Documentation of the implementation used can be found in Appendix \ref{appendix:impl}, and the code, including several demo's is available on www.github.nl/dieuwkehupkes. In this section we will discuss the results of our experiments, and some difficulties that arose in obtaining them.


\subsection{Direct Similarity}

We measured the consistency of the corpus through similarity metric \ref{met:depHAT}. The results scores of the four automatically aligned datasets can be found in Table \ref{tab:scores1}.

\begin{table}[!h]
\centering
\begin{tabular}{l|ccc}
& \multicolumn{3}{c}{Scores}\\
Language pair & |s| < 10 & |s| < 20 & |s| < 40\\
\hline
English-Dutch & 0.47 & 0.42 & 0.40 \\
English-French & 0.46 & 0.42 & 041 \\
English-German & 0.44 & 0.41 & 0.38 \\
English-Chinese & 0.59 & 0.48 & 0.42\\
\end{tabular}
\caption{Results Experiment 1, automatic alignments}\label{tab:scores1}
\end{table}

As expected, the scores are rather low, due to discrepancy between the relatively flat dependency parses and maximally compositional translation structures. The scores for the manually aligned corpora, that are not much higher, can be found in Table \ref{tab:scores2}. Unfortunately, the manually aligned datasets are not identical to the 

\begin{table}[!h]
\centering
\begin{tabular}{l|ccc}
& \multicolumn{3}{c}{Scores}\\
Language pair & |s| < 10 & |s| < 20 & |s| < 40\\
\hline
English-German (Pado) & 0.43 & 0.42 & 0.41 \\
English-French (Hansard) & 0.63 & 0.54 & 0.51 \\
English-French (LREC) & 0.49 & 0.47 & 0.47 \\
English-Portuguese (LREC) & 0.47 & 0.45 & 0.45 \\
English- Spanish (LREC) & 0.51 & 0.48 & 0.48\\
\end{tabular}
\caption{Results Experiment 1, Manual Alignments}\label{tab:scores2}
\end{table}

\subsection{Completer Similarity ??}

The results of our second experiment, in which we allows arguments of dependent heads to combine in different stages, are of more interest, as they provide a more thorough picture of the similarity between the two types of compositionality. As expected, the scores are much higher, as can be seen in Table \ref{tab:scores3}.

\begin{table}[!h]
\centering
\begin{tabular}{l|ccc}
& \multicolumn{3}{c}{Scores}\\
Language pair & |s| < 10 & |s| < 20 & |s| < 40\\
\hline
English-Dutch & 0.79 & 0.74 & 0.71 \\
English-French & 0.80 & 0.77 & 0.76\\
English-German & 0.75 & 0.71 & 0.68 \\
English-Chinese & 0.76 & 0.67 & 0.62\\
\end{tabular}
\caption{Results Experiment 2, automatic alignments}\label{tab:scores3}
\end{table}

blabla

\begin{table}[!h]
\centering
\begin{tabular}{l|ccc}
& \multicolumn{3}{c}{Scores}\\
Language pair & |s| < 10 & |s| < 20 & |s| < 40\\
\hline
English-German (Pado) & 0.82 & 0.80 & 0.77 \\
English-French (Hansard) & 0.85 & 0.80 & 0.78 \\
English-French (LREC) & 0.78 & 0.82 & 0.82 \\
English-Portuguese (LREC) & 0.75 & 0.76 & 0.76 \\
English- Spanish (LREC) & 0.79 & 0.80 & 0.80\\
\end{tabular}
\caption{Results Experiment 2, Manual Alignments}\label{tab:scores2}
\end{table}


\appendix
\chapter{Implementation}
\label{appendix:impl}

\chapter{Manual Analysis}
\label{appendix:analysis}



\bibliography{thesisDH}
\end{document}