\documentclass{report}
\input{commands.tex}

\begin{document}

\chapter{Experimental Results}

Introduction.
This chapter will consist of two main parts. Firstly, we will report on our findings of the similarity between dependency parses and translation data. blabla. In the second part, we will try to develop our own grammar, using dependency grammars, ...



\section{Data}

\subsection{Corpora}

We had available 4 large automatically aligned parallel corpora for the language pairs English-Dutch, English-French, English-German and English-Chinese. The first thee corpora were data from the European Parliament taken from the Europarl corpus \citep{koehn2005europarl}, while the English-Chinese data came from the Hong Kong Parallel Corpus. The word-alignments were found using GIZA++ \citep{och03:asc}, using the previously mentioned `grow-diag-and-final'-heuristic, with 4 iterations on model 1, and 3 iterations with the hmm model, model 3 and model 4. The corpora were tokenised and lowercased before GIZA++ was run. In general, the guidelines for building a baseline for the WMT workshops were followed.\footnote{See http://www.statmt.org/wmt07/baseline.html}

In addition to the automatically aligned corpora, we had access to 5 (much smaller) manually aligned corpora, for the language pairs English-French, \citep{graca2008building,och2000improved} English-Spanish, English-Portuguese \citep{graca2008building}, English-German \citep{pado2006optimal}.

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\bf Language pair & \bf Source & \bf Size & \bf Alignments\\
\hline \hline
English - Dutch & European Parliament & 945167 & {\small GIZA++} \\
\hline
English - German & European Parliament & 995909 & {\small GIZA++} \\
& Europarl & 987 & {\small\cite{pado2006optimal}}\\
\hline
English - Chinese & Hong Kong Parallel Corpus & 1723487 & {\small GIZA++} \\
\hline
English - French & European Parliament & 949408 & {\small GIZA++} \\
& Hansard & 447 & {\small \cite{och2000improved}}\\
& Europarl & 100 & {\small \cite{graca2008building}} \\
\hline
English - Spanish & Europarl & 100 & {\small \cite{graca2008building}} \\
\hline
English - Portuguese & Europarl & 100 & {\small \cite{graca2008building}}\\
\hline
\end{tabular}
\caption{Datasets}\label{tab:datasets}
\end{table}

\subsection{Parses}

The English side of the corpus was parsed with the Stanford dependency parser \citep{de2008stanford}, using new lines as delimitation to respect the sentence alignment of the corpora, to obtain basic typed dependencies of the sentence.  The set of dependencies considered by the parser does not include dependencies between words and punctuation (or punctuation and punctuation). More information on the different dependencies generated by the Stanford parser can be found in \cite{de2006generating}.

\section{Direct Similarity of Alignments and Dependency parses}

We measured the consistency of the corpus through similarity metric \ref{met:depHAT}. The results scores of the four automatically aligned datasets can be found in Table \ref{tab:scores1}, a graph with the distribution of the different scores of the automatic datasets is depicted in Figure \ref{fig:scoredstrib}. As expected, the dependency parses do not correspond very well with the alignments in such a direct fashion. The difference between the two French manually aligned datasets is surprisingly high, from which we can conclude blabla not big enough to be sigificant ...

%Also put nr of sentences in the table, + change to numbers of larger dataset
%Change tabel styles
%Hansard:{40: 148, 10: 97, 20: 202}
%LREC Frans {40: 0, 10: 29, 20: 71}
%LREC Portuguese {40: 0, 10: 29, 20: 71}
%LREC Spanish {40: 0, 10: 29, 20: 71}
%Pado {40: 632, 10: 63, 20: 275}
\begin{table}[!h]
\centering
\begin{tabular}{l|ccc}
& \multicolumn{3}{c}{\textbf{Scores}}\\
\textbf{Language pair} & |s| < 10 & |s| < 20 & |s| < 40\\
\hline
\multicolumn{4}{l}{Automatic Alignments}\\
\hline
English-Dutch & \textbf{0.47} & \textbf{0.42} & \textbf{0.40} \\
English-French & 0.46 & 0.42 & 041 \\
English-German & \textbf{0.44} & 0.41 & 0.38 \\
English-Chinese & \textbf{0.59} & \textbf{0.48} & \textbf{0.42}\\
\hline
\multicolumn{4}{l}{Manual Alignments}\\
\hline
English-French (Hansard) & \textbf{0.63} & \textbf{0.54} & \textbf{0.51} \\
English-French (LREC) & 0.49 & 0.47 & 0.47 \\
English-German (Pado) & 0.43 & \textbf{0.42} & \textbf{0.41} \\
English-Portuguese (LREC) & \textbf{0.47} & \textbf{0.45} & \textbf{0.45} \\
English- Spanish (LREC) & \textbf{0.51} & \textbf{0.48} & \textbf{0.48}\\
\end{tabular}
\caption{Results Experiment 1, automatic alignments}\label{tab:scores1}
\end{table}



\begin{figure}
\input{score_distribution.tex}[!ht]
\caption{Distribution of the scores}\label{fig:scoredstrib}
\end{figure}


\section{Deeper Similarity of Alignments and Dependency Parses}



\begin{table}[!h]
\centering
\begin{tabular}{l|ccc}
& \multicolumn{3}{c}{\textbf{Scores}}\\
\textbf{Language pair} & |s| < 10 & |s| < 20 & |s| < 40\\
\hline
\multicolumn{4}{l}{Automatic Alignments}\\
\hline
English-Dutch & 0.79 & 0.74 & 0.71 \\
English-French & 0.80 & 0.77 & 0.76\\
English-German & 0.75 & 0.71 & 0.68 \\
English-Chinese & 0.76 & 0.67 & 0.62\\
\hline
\multicolumn{4}{l}{Manual Alignments}\\
\hline
English-French (Hansard) & 0.85 & 0.80 & 0.78 \\
English-French (LREC) & 0.78 & 0.82 & 0.82 \\
English-German (Pado) & 0.82 & 0.80 & 0.77 \\
English-Portuguese (LREC) & 0.75 & 0.76 & 0.76 \\
English- Spanish (LREC) & 0.79 & 0.80 & 0.80\\
\end{tabular}
\caption{Results Experiment 2, automatic alignments}\label{tab:scores3}
\end{table}




Briefly discuss some more reasons

\subsection{Reasons for deviation}

blablabla manual analysis of the manually aligned datasets. The only languages within this set known to the author of this paper are French and German... bla introduction

\subsubsection{German}

\subsubsection{Frans}

\subsubsection{Match up with rules?}

distribution of rules??

\section{A more probabilistic Approach}

Explain how we have matched the two types of compositionality, was not completely consistent. To learn where it deviates, and what would best fit the entire corpus, we can \textit{learn} the grammar for the entire corpus, making use of dependency labels. By using dependency labels, the link with monolingual grammars stays in tact, but the learn algorithm can learn where to make adaptations, instead of always following dependency parses.

Read of the grammar for entire corpus, parse entire corpus, read of grammar again


\section{}








\bibliography{thesisDH}
\end{document}