\documentclass{article}
\input{commands.tex}

\newcommand{\G}{\mathcal{G}}
\newcommand{\Hg}{\mathcal{H}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\Pp}{\mathcal{P}}


\begin{document}

\section{EM for HATS using ML}

In this section, I describe how perform EM with HATs represented as CFG's, based on Maximum likelihood estimation. We will explain the basics, and provide algorithms executing the iterative step in a fashion that not all memory needs to be accessible all at once, and recomputing earlier computed information is avoided.

\subsection{Basics}

We have a set of HATS for every sentence in a corpus, and we have a CFG that contains the rules of all these HATS. However, we do not know the probability distribution of the HATS over the sentences, nor the weights that should be assigned to the CFG. As we could compute the probability distribution over the HATs if we had the weights of the PCFG, and vice versa, this is a problem of incomplete data, that can be addressed with the Expectation Maximisation algorithm.

We initialise by setting the distribution over the HATs to uniform, by counting the number of rules and normalising the result, we create our first grammar $\G$. We then proceed as follows:\begin{enumerate}
\item Compute the probabilities of all HATS for all sentences according to $\G$ 
\item Create a new grammar
	\begin{enumerate}
		\item[1.1] for every sentence:
			\begin{itemize}
				\item Normalise the probabilities of its HATs such that they sum up to one
				\item For each HAT, multiply the rules in the HAT with the probability of the HAT, and add increase the current count of the rule in $\G_{new}$ with this number
			\end{itemize}
		\item[1.2] Normalise $\G_{new}$, set $\G = \G_{new}$
	\end{enumerate}
\item Go back to step 1, or stop when converging or a maximum number of iterations is reached. 
\end{enumerate}

The crucial step in this algorithm, is finding the updates for every sentence. The algorithm for doing so is presented in this section. In the next subsection, we will give an example.

\subsection{EM for HATS, worked out example}

Consider the following HAT forest:

\begin{enumerate}
\item[] 1. \Tree [.A 0 [.B [.G 1 2 ] [.D 4 3 ] ] ] \hfill 2. \Tree [.A [.C 0 [.G 1 2 ] ] [.D 4 3 ] ] \hfill 3.\Tree [.A [.F 0 1 ] [.E 2 [.D 4 3 ] ] ]
\item[]
\item[] 4. \Tree [.A 0 [.B 1 [.E 2 [.D 4 3 ] ] ] ] \hfill 5. \Tree [.A [.C [.F 0 1 ] 2 ] [.D 4 3 ] ]
\end{enumerate}

The PCFG we are training has the following rules that are of interest for this HATforest:

\begin{table}[!ht]
\begin{tabular}{llllllll}
A $\rightarrow$ 0 B & 0.5 && C $\rightarrow$ 0 G & 0.4 && G $\rightarrow$ 1 2 & 0.75\\
A $\rightarrow$ C D & 0.2 && C $\rightarrow$ F 2 & 0.8 && \\
A $\rightarrow$ F E & 0.3 && D $\rightarrow$ 4 3 & 0.1 && \\
B $\rightarrow$ G D & 0.8 && E $\rightarrow$ 2 D & 0.1 &&\\
B $\rightarrow$ 1 E & 0.25 && F $\rightarrow$ 0 1 & 0.5 &&\\
\end{tabular}
\end{table}

\noindent Which means we get the following probabilities for the 5 trees:
\begin{align*}
&P(T_1) = P(A\rightarrow 0~B)\cdot P(B\rightarrow G~D)\cdot P(G\rightarrow 1~2)\cdot P(D\rightarrow 4~3)= 0.5 \cdot 0.8 \cdot 0.75 \cdot 0.1 = 0.03 \\
&P(T_2)= P(A\rightarrow C~D)\cdot P(C\rightarrow 0~G)\cdot P(G\rightarrow 1~2)\cdot P(D\rightarrow 4~3) = 0.2\cdot 0.4\cdot 0.75\cdot 0.1 = 0.006 \\
&P(T_3) = P(A~\rightarrow F~E)\cdot P(F\rightarrow 0~1)\cdot P(E\rightarrow 2~D)\cdot P(D\rightarrow 4~3) = 0.3\cdot 0.5\cdot 0.1 \cdot 0.1 = 0.0015 \\
&P(T_4) = P(A~\rightarrow~0~B)\cdot P(B\rightarrow 1~E)\cdot P(E\rightarrow 2~D)\cdot P(D\rightarrow 4~3) = 0.5 \cdot 0.25\cdot 0.1 \cdot 0.1 = 0.00125\\
&P(T_5) = P(A~\rightarrow~C~D)\cdot P(C\rightarrow F~2)\cdot P(F\rightarrow 0~1)\cdot P(D\rightarrow 4~3) = 0.2\cdot 0.8\cdot 0.5\cdot 0.1 = 0.008\\
\end{align*}

\noindent The normalisation factor $\N$ is thus $0.03+0.006+0.0015+0.00125+0.008 = 0.04675$. We can now recompute the contributions of the tree to to grammar, by counting how often rules occur in the trees:

\begin{align*}
&\mathcal{C}(A\rightarrow 0~B) = \N\cdot(0.03 + 0.00125) = \N\cdot(0.03125) = 0.67\\
&\mathcal{C}(A\rightarrow C~D) = \N\cdot(0.006 + 0.008) = \N\cdot(0.014) = 0.30\\
&\mathcal{C}(A\rightarrow F~E) = \N\cdot(0.0015)= 0.032 \\
&\mathcal{C}(B \rightarrow G~D) = \N\cdot(0.03)= 0.64\\
&\mathcal{C}(B \rightarrow 1~E) = \N\cdot(0.00125) = 0.027\\
&\mathcal{C}(C \rightarrow 0~G) = \N\cdot(0.006) = 0.13\\
&\mathcal{C}(C \rightarrow F~2) = \N\cdot(0.008) = 0.17\\
&\mathcal{C}(D\rightarrow 4 3) = \N\cdot(0.03 + 0.006 + 0.0015 + 0.00125 + 0.008) = \N\cdot(0.04675) = 1\\
&\mathcal{C}(E\rightarrow 2~D) = \N\cdot(0.0015 + 0.00125) = \N\cdot(0.00275) = 0.059\\
&\mathcal{C}(F\rightarrow 0~1) = \N\cdot(0.0015 + 0.008) = \N\cdot(0.0095) = 0.20\\
&\mathcal{C}(G\rightarrow 1~2) = \N\cdot(0.03 + 0.006) = \N\cdot(0.036) = 0.77\\
\end{align*}



\subsection{Labelling}

As was apparent from the examples, the HATs need to be labelled to be able to detect patterns and compare rules across HATs. We will label the HATs based on dependency parses, that provide labels for a small subset of all spans.

\subsubsection{SAMT}

A method to create more labels from a set of basic labels was proposed in (ref). A span $(i,j)$ was labelled by following the following plan ($A$ and $B$ always refer to basic labels):

\begin{enumerate}
\item if possible, label $(i,j)$, with a basic label
\item elif if possible, assign $(i,j)$ a label of the form $A+B$
\item elif if possible, assign $(i,j)$ a label of the form $A/B$ or $A\backslash B$
\item elif if possible, assign $(i,j)$ a label of the form $A+B+C$
\item else assign $(i,j)$ the label FAIL.
\end{enumerate}

We have tested the success rate of the spans in the HATs for the first 10.000 sentences of the 4 manually aligned datasets, the result is depicted in Table \ref{tab:SAMT}.

\begin{table}[!ht]
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Language pair} & \textbf{Spans total} & \textbf{Spans labelled SAMT} & \textbf{\% Labelled}\\
\hline \hline
English-Dutch & 1078307 & 598583 & 0.56\\
\hline
English-French & 1260720 & 644710 & 0.51\\
\hline
English-German & 930273 & 540968 & 0.58\\
\hline
English-Chinese & 609401 & 375765 & 0.62\\
\hline
\end{tabular}
\caption{Success rate of SAMT labels}\label{tab:SAMT}
\end{table}

Clearly, this not high enough to be useful for our purpose. We extended the SAMT labels with more labels, until all spans were labelled, according to the following protocol (A,B and C once again refer to basic labels):

\begin{enumerate}
\item $n = 1$
\item try to find a label of the form $A_1+\cdots+A_m$ with $m =n$
\item try to find a label of the form $A/B_1+\cdots+B_m$ or $B_1+\cdots+B_m\backslash A$ with $m+1=n$ variables
\item try to find a label of the form $A_1+\cdots+A_k\backslash B/C_1+\cdots+C_m$ with $m+k+1=n$ 
\item $n\leftarrow n+1$, go back to 1.
\end{enumerate}

\noindent Note that it is possible (and even plausible) that different nodes in one HAT get the same label. However, the labels in a CFG describing a HATforest, must be unambiguous. In practice, we will often annotate the labels with spans if knowledge about the spans labels are covering is required. We will omit this annotation in the following descriptions.

\subsection{Computation}

As our HATs are implicitly represented as CFG's, rather than as complete sets of trees, we cannot directly follow the previously described protocol, as this would require unpacking the entire HATforest, which would be both time and space consuming. In this subsection, we will present an algorithm to compute the updates for a HATforest to a grammar, that focusses on reusing previously computed values, and using only a limited amount of memory at one moment. We will use the following abbreviations:\begin{enumerate}
\item $P(A)$ is the sum of the probabilities of all subtrees of HATs headed by $A$
\item $P(A_{x_1\cdots x_n})$ is the sum of the probabilities of all subtrees of HATs headed by $A (x_1 \cdot x_n)$
\end{enumerate}

To find the updates for a rule, we need to know how often it appeared in the HATforest, and what probability the trees it occurred in HAT. We will obtain this information by going through the HATforest in a topdown fashion, and compute the probability mass of the trees a rule occurred in using the relative probability mass of its different expansions, and the probability mass of all subtrees headed by the parent of the lefthandside of the rule.

The update process starts by calling the update function (Algorithm \ref{alg:update}) on the topnode of the HATforest, which will make new function calls while computing the probabilities of the productions with this topnode as left-hand side. Counts for productions can be updated multiple times for each rule. Note that the algorithm does not return anything, but merely updates a global dictionary with counts for rules.

\begin{algorithm}
\caption{$update(N,\Hg,\G,\Pp,\C,p_{total})$}\label{alg:update}
\begin{algorithmic}
\STATE \textbf{Input:} a HAtforest $\Hg$, a global grammar $\G$, a function $\Pp$, the current counts $\C$, a node $N$ and a number $p_{total}$ describing the probability mass that is assigned to the parent $N$
\STATE
\IF{$\neg \exists N\rightarrow X_1~\cdots X_n\in\Hg$}
	\RETURN
\ENDIF
\FOR{$R = N\rightarrow X_1~\cdots X_n\in\Hg$}
	\STATE $C_{new} = \frac{\Pp(N_{X_1\cdots X_n})}{\sum_{R} \Pp(N_{X_1\cdots X_n})}$
	\STATE $\C(N\rightarrow X_1\cdots X_n) \leftarrow \C(N\rightarrow X_1\cdots X_n) + C_{new}  $
	\FOR{$X\in \{X_1,\cdots X_n\}$}
		\STATE $update(X,\Hg,\G,\Pp,\C,C_{new}))$
	\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:update} uses  a function $\Pp$, that can be computed before updating starts, also in a top-down recursive fashion, as is described in Algorithm \ref{alg:prob}. The entire function is computed by calling $prob$ on the topnode of the HATforest, where the function will call itself to compute all its values. Naturally, when the function encounters a call it already saw before, it does not recompute its outcome, but uses the previously found information.

\begin{algorithm}[!ht]
\caption{$prob(\Pp, \Hg,\G,X,(y_1,\cdots,y_n) = None$)}\label{alg:prob}
\begin{algorithmic}
\STATE \textbf{Input:} a dictionary with already computed probabilities $\Pp$, a HATgrammar $\Hg$, a global PCFG $\G$, a node $X$ in $\Hg$, and optionally a set of children in $(y_1\cdots y_n)$ such that $X\rightarrow y_1~\cdots y_n$ is a rule in $\Hg$
\STATE
\IF{$X \in \Pp$}
	\RETURN $\Pp(X)$
	\STATE
\ELSIF{$\neg\exists X\rightarrow C_1~\cdots ~C_N \in\Hg$}
	\RETURN $\Pp(X)=1$
	\STATE
\ELSIF{$(y_1\cdots y_n)!=$None}
	\RETURN $\Pp(X_{y_1\cdots y_n}) = \G(X\rightarrow y_1~\cdots y_n)\cdot\prod_{n}prob(\Hg,\G,y_n)$
	\STATE
\ELSE
	\RETURN $\sum_{X\rightarrow y_1~\cdots ~y_n \in\Hg} \Pp(X_{y_1~\cdots y_n})$
	\STATE
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsection{Computation EM for HATs, example}

For illustration purposes, we will show how to compute the updates for the running example.

\subsubsection{Compute $\Pp$}

Firstly, we compute the function $\Pp$, which starts by calling $prob(A)$:
\begin{align*}
\mathbf{prob(A)}&\mathbf{= prob(A_{0B}) + prob(A_{CD}) + prob(A_{FE})}\\
prob(A_{0B}) &= \G(A\rightarrow 0~B)\cdot prob(0) \cdot prob(B) &= 0.5 \cdot prob(0) \cdot prob(B)\\
prob(A_{CD}) &= \G(A\rightarrow C~D)\cdot prob(C) \cdot prob(D) &= 0.2 \cdot prob(C) \cdot prob(D)\\
prob(A_{FE}) &= \G(A\rightarrow F~E)\cdot prob(F) \cdot prob(E) &= 0.3 \cdot prob(F) \cdot prob(E)
\end{align*}

\noindent We work further down the tree, computing $prob(B)$ and $prob(C)$, $prob(D)$, $prob(F)$ and $prob(E)$:
\begin{align*}
\mathbf{prob(B)}&= \mathbf{prob(B_{GD}) + prob(B_{1E})}\\
prob(B_{GD}) &= \G(B\rightarrow G~D)\cdot prob(G) \cdot prob(D) &= 0.8 \cdot prob(G) \cdot prob(D)\\
prob(B_{1E}) &= \G(B\rightarrow 1~E)\cdot prob(1) \cdot prob(E) &= 0.25 \cdot prob(0) \cdot prob(E)\\
\\
\mathbf{prob(C)}&\mathbf{= prob(B_{GD}) + prob(B_{1E})}\\
prob(C_{0G}) &= \G(C\rightarrow 0~G)\cdot prob(0) \cdot prob(G) &= 0.4 \cdot prob(0) \cdot prob(G)\\
prob(C_{F2}) &= \G(C\rightarrow F~2)\cdot prob(F) \cdot prob(2) & = 0.8 \cdot prob(F) \cdot prob(2)\\
\\
\mathbf{prob(D)}&\mathbf{= prob(D_{34})}\\
prob(D_{43}) &= \G(D\rightarrow 3~4) \cdot prob(3) \cdot prob (4) &= 0.1 \cdot prob(3) \cdot prob(4)\\
\\
\mathbf{prob(F)}&\mathbf{= prob(F_{01})}\\
prob(F_{01}) &= \G(F\rightarrow 0~1) \cdot prob(0) \cdot prob (1) &= 0.5 \cdot prob(0) \cdot prob (1) \\
\\
\mathbf{prob(E)}&\mathbf{= prob(E_{2D})}\\
prob(E_{2D}) &= \G(E\rightarrow 2~D) \cdot prob(2) \cdot prob(D) &= 0.1\cdot prob(2) \cdot prob(D)
\end{align*}

\noindent After computing $prob(0)=prob(1) = prob(2) = prob(3) = prob(4) = 1$ and $P(G) = P(G_{12}) = 0.75$ we get all the values of the function:
\begin{align*}
\Pp(A) &= 0.04675 & 	\Pp(A_{0B}) &= 0.03125 &	\Pp(A_{CD}) &= 0.014 &	 \Pp(A_{FE}) &= 0.0015 \\
\Pp(B) &= 0.0625 & 	\Pp(B_{GD}) &=0.06 &	 	\Pp(B_{1E}) &= 0.0025\\
\Pp(C) &= 0.7 & 		\Pp(C_{0G}) &= 0.3 & 		\Pp(C_{F2}) &= 0.4\\
\Pp(D) &= 0.1 & 		\Pp(D_{34}) &= 0.1 \\
\Pp(E) &= 0.1 & 		\Pp(E_{2_D})&= 0.01 \\
\Pp(F) &=  0.5 & 		\Pp(F_{01}) &= 0.5 \\
\Pp(G) &= 0.75 & 		\Pp(G_{12}) &= 0.75
\end{align*}

\subsection{Compute updates}

With this information, we can compute the updates for the counts. Once again, the recursive computation is invoked by calling the function on the topnode $A$. In our example we will again omit input variables $\N$, $\Hg$, $\G$, $\C$ and $\Pp$. The process starts once again by calling the update function on $A$, as $A$ covers all trees, the total probability mass is $1$, we thus start by computing $update(A,1)$:

\begin{align*}
\C(A\rightarrow 0~B) &\leftarrow 0 + \frac{P(A_{0B})}{P(A_{0B})+P_(A_{CD})+P(A_{FE})} &= \frac{0.03125}{0.04675} &= 0.67\\
\C(A\rightarrow C~D) &\leftarrow 0 + \frac{P(A_{CD})}{P(A_{0B})+P_(A_{CD})+P(A_{FE})} &= \frac{0.014}{0.04675} &= 0.30\\
\C(A\rightarrow F~E) &\leftarrow 0 + \frac{P(A_{FE})}{P(A_{0B})+P_(A_{CD})+P(A_{FE})} &= \frac{0.0015}{0.04675} &= 0.032
\end{align*}
The following function calls are made: $update(B,0.67)$, $update(C,0.30)$, $update(D,0.30)$, $update(F,0.032)$, $update(E,0.032)$ (also $update(0,0.67)$ is called, but will return directly, becuase $0$ does not further expand).
\begin{align*}
\mathbf{update(B,0.67)}\\
\C(B\rightarrow G~D) &\leftarrow 0 + 0.67\cdot\frac{P(B_{GD})}{P(B_{GD})+P(B_{1E}} &= 0.67 \cdot\frac{0.06}{0.0625} &= 0.64\\
\C(B\rightarrow 1~E) &\leftarrow 0 + 0.67\cdot\frac{P(B_{1E})}{P(B_{GD})+P(B_{1E}} &= 0.67\cdot \frac{0.0025}{0.0625} &= 0.027
\end{align*}
New function calls: $update(G,0.64)$, $update(D,0.64)$, $update(E,0.027)$
\begin{align*}
\mathbf{update(C,0.30)}\\
\C(C\rightarrow 0~G) &\leftarrow 0 + 0.30\cdot\frac{P(C_{0G})}{P(C_{0G})+P(C_{F2})} &= 0.30\cdot\frac{0.3}{0.7}) &= 0.13\\
\C(C\rightarrow F~2) &\leftarrow 0 + 0.30\cdot\frac{P(C_{F2})}{P(C_{0G})+P(C_{F2})} &= 0.30\cdot\frac{0.4}{0.7} &= 0.17
\end{align*}
New function calls: $update(G,0.13)$, $update(F,0.17)$
\begin{align*}
\mathbf{update(D,0.30)}\\
\C(D\rightarrow 4~3) &\leftarrow 0 + 0.30\cdot \frac{P(D_{34})}{P(D_{34})} &  &= 0.30\\
\mathbf{update(F,0.032)}\\
\C(F\rightarrow 0~1) &\leftarrow 0 + 0.032\cdot \frac{P(F_{01})}{P(F_{01})} &&= 0.032\\
\mathbf{update(E,0,032)}\\
\C(E\rightarrow 2~D) &\leftarrow 0 + 0.032\cdot \frac{P(E_{2D}}{P(E_{2D}} &&= 0.032
\end{align*}
New function call: $update(D,0.032)$. Function calls waiting are thus: $update(G,0.64)$, $update(D,0.64)$, $update(E,0.027)$, $update(G,0.13)$, $update(F,0.17)$ and $update(D,0.032)$
\begin{align*}
\mathbf{update(G,0.64)}\\
\C(G\rightarrow 1~2) &\leftarrow 0 + 0.64 \cdot \frac{P(G_{12})}{P(G_{12})} &= 0.64\\
\mathbf{update(D,0.64)}\\
\C(D\rightarrow 4~3) &\leftarrow 0.30 + 0.64\cdot \frac{P(D_{34})}{P(D_{34})} &= 0.94\\
\mathbf{update(E,0.027)}\\
\C(E\rightarrow 1~D) &\leftarrow 0.032 + 0.027\cdot \frac{P(E_{1D})}{P(E_{1D})} &= 0.059
\end{align*}
New function call: $update(D,0.027)$
\begin{align*}
\mathbf{update(G,0.13)}\\
\C(G\rightarrow 1~2) &\leftarrow 0.64 + 0.13 \cdot \frac{P(G_{12})}{P(G_{12})} = 0.77\\
\mathbf{update(F,0.17)}\\
\C(F\rightarrow 0~1) &\leftarrow 0.17 + 0.032\cdot \frac{P(F_{01})}{P(F_{01})} &= 0.20\\
\mathbf{update(D,0.032)}\\
\C(D\rightarrow 4~3) &\leftarrow 0.64 + 0.032\cdot \frac{P(D_{34})}{P(D_{34})} &= 0.97\\
\mathbf{update(G,0.13)}
\C(G\rightarrow 1~2) &\leftarrow  0.64 \cdot 0.13 \frac{P(G_{12})}{P(G_{12})} &= 0.77\\
\mathbf{update(D,0.059)}\\
\C(D\rightarrow 4~3) &\leftarrow 0.97 + 0.027\cdot \frac{P(D_{34})}{P(D_{34})} &= 1\\
\end{align*}

\end{document}
