\documentclass{report}
\input{commands.tex}


\begin{document}

\chapter{A new empirical Investigation of Compositionality}

In the previous chapter, we have introduced the problems concerning compositional translation, and we have seen that the matter is far from resolved. 

\begin{enumerate}
\item in previous chapter we argued that empirical analysis is necessary
\item we have introduced the ingredients necessary for empirical analysis
\item we have discussed existing empirical analyses, that...
\item furthermore, we observed that in the literature no empirical analysis can be found that combines the monolingual and bilingual perspective.
\item in this thesis..., both a theoretical and practical purpose
\item in this chapter...
\item overview
\end{enumerate}

We can conclude that compositional translation for natural languages is far from sorted out (anders). Many assumptions are made on the way, and the extent to which these assumptions are true is unknown. In this thesis, we will try to find evidence for how compositional translation of natural language is. We will assume that translation is compositional, and investigate how complex and specific the rules and the parts of this system are. For such a study, an idiomatic expression or a strange phenomenon sitting in a corner of language is no evidence against compositionality, rather, we want to know how many phenomena and expressions are sitting in this corner, and if we can add all these phenomena and expressions to a compositional grammar without loosing the feeling that overall translations were compositional.


We will focus on HATs
We will study the consitency of HATs and dependency grammars, and study the main causes of deviation of the translation structures from conventional linguistic syntax. We will present an easily accessible implementation, that is reusable for various purposes and is thus suitable for further investigation by others. In this chapter, we motivate and explain the procedures that were used, and .... We will start by discussing our translation structures... on a more intuitive level.


\section{Foundations of Empirical Studies}

The analysis of compositionality presented in this thesis is based on real data, that are not always perfect. To appreciate empirical research, it is important to have a good picture of the assumptions that are made about this data, and the influence they might have on the result. In this section, we will mention and discuss these assumptions, some of which may sound obvious, to provide a complete picture of the foundations of empirical studies.


\subsection{Correctness of the Translation Data}

Our empirical analysis is based on parallel corpora with text that are each others translation, and thus relies heavily on the correctness of these corpora. As these parallel texts were not designed as data for translation models, they might not be perfectly suitable for this purpose. When training MT models, infrequent mistakes in the data are generally not problematic, as they will receive a low probability. This is not the case in an empirical analysis. There are three bottlenecks:

\subsubsection{Sentence level alignment.}
Aligning corpora on the sentence level is not as simple as it might seem. Texts are not always translated sentence by sentence. Short sentences may be merged or long ones broken up, and in some languages sentence delimiters do not really exist \citep[p.55]{koehn2008statistical}. However, the techniques for sentence alignment are very good, and as the languages we are considering \textit{do} have clear sentence delimiters it seems very reasonable to assume that the sentences in the corpora are correctly aligned.

\subsubsection{Correctness of translation.}
The translations of the sentences are produced by humans, who sometimes make mistakes. To use the corpora, we have to assume that the aligned sentences are good translations of each other.

\subsubsection{Translation is literal.}
One English sentence often has many translations in another language, as similar meanings can be expressed in multiple ways.\footnote{In fact, considering only one target translation is thus also a simplification made in empirical research} For instance `jeg giver dig blomster' is a good Danish translation of `I give you flowers', but so is `jeg giver blomster til dig' (and the amount of rephrasing in this example is even rather minimal). Especially when one text is not a direct translation of the other text, but the two are, for instance, just separate reports of the same event, it might happen that sentences do have the same meaning, but are not very similar in form. In our analysis, we will assume that at least the vast majority of the translations in the corpora are rather literal.
 
\subsection{Correctness Word Alignments}

In empirical analyses, as well as MT-models, word-alignments are used to establish translational correspondences, and are thus of crucial importance. Unfortunately, automatic alignments are not always as good as we want them to be. MT-models generally do not suffer much from this fact, because the number of untrue alignment links is dwarfed by the number that is correct. For empirical research, false alignment links are quite problematic, as even one wrong link can have a huge effect on the space of possible translation trees. An option is to use one of the few manually aligned corpora, but given their small size they are not suitable to draw conclusions about larger parts of language.

\subsection{Dependency Parser}

To determine the dependency structures of the sentences in the corpus, an efficient fully automated dependency parser is needed.  For English, high quality dependency parses are available \citep{cer2010parsing}, but the parses they produce are not perfect, which can be problematic for an empirical analysis.

\subsection{Conclusion?}

Although statistical analysis of the corpora might provide more insight in the type of mistakes that are generally made, the type of mistakes sketched in the previous subsections will harm the outcome of an empirical analysis, making it more pessimistic than it would be if the data were perfect. It therefore seems sensible to see empirical analysis as an upper- or lowerbound (depending on the context), rather than an exact number.



\section{Translation Structures}



Recall: translations structures
In this thesis, we will consider a subset of all alignment trees, that was already mentioned in the previous chapter: the HATs \citep{simaan2013hats}. A HAT is characterized by the fact that it is maximal, i.e., every node has its minimal branching factor.\footnote{Say something about node operators and skeleton} This means that every non-terminal node is expanded into a minimum number of children, but also that phrases are used \textit{only} when they are needed to account for the translation phenomena. An example that illustrates this is depicted in Figure \ref{fig:treephrase}.

\begin{figure}
\centering
\begin{tikzpicture}

\node (je) at (0,0.07) {Je};
\node (ne) at (0.5,0.04) {ne};
\node (fume) at (1.2,0.08) {fume};
\node (pas) at (2,0) {pas};

\coordinate (nepas) at (1.4,0.7);
\coordinate (jefume) at (0.6,0.7);
\coordinate (all) at (1.0,1.1);

\draw (nepas) -- (ne);
\draw (nepas) -- (pas);
\draw (je) -- (all);
\draw (fume) -- (all);
\draw (all) -- (nepas);

\end{tikzpicture}
\caption{Describe how a phrase-based translation system would consider `ne fume pas' als een phrase pair, and the underlying structure would be gone, while the HAT preserves the internal structure}\label{fig:treephrase}
\end{figure}


\subsection{Motivation}

There is a computational advantage to considering only minimally branching trees. Not only does it significantly reduce the space of trees to be considered (in case of the previous dog example, there are 5 trees instead of 44), it also simplifies parsing, as the lower-rank rules that can be extracted from minimally branching trees can be more efficiently treated by a parsing algorithms. Such considerations are very important when designing an MT model. Although it is even for an empirical analysis that the computational requirements match the reality, one might doubt whether restricting the set of trees to consider is theoretically justifiable. In the following paragraphs we will argue it is.

First of all, considering only maximally branching trees secures that the system we are studying is in fact compositional. The set of all compositional translation trees contains many flat trees, that can strictly speaking be seen as compositional (as compositionality is highly underspecified in this respect), but do not capture the recursive and systematic nature of language. A compositional system containing a separate rule for almost every sentence that specifies how the meaning can be derived from \textit{all of its words} does certainly not correspond with human intuitions about compositionality, not to mention the fact that such a grammar should be infinite to cover the entire language. Considering only minimally branching trees solves this problem.

Secondly, considering only expansions that are maximally compositional maximises the chanse that generalisation to new data is possible: a rule that specifies how a type of argument can be combined with a type of predicate is more useful than a rule specifying how the argument `I' can be combined with the predicate `like'. Minimum depth expansions are more probable to be applicable in new situations.

A special case that ties together the previous two arguments occurs with the treatment of phrasal translations. As the rest of the tree, sequences of words will be translated as a whole only if they do not have a deeper structure according to the translation data. In many phrase-based translation systems, including the successful hierarchical phrase based translation, the underlying system of sequences that are translated phrasally gets lost in the process, whereby the system misses an opportunity to detect a pattern. Figure \ref{fig:treephrase} illustrates the advantage of this property.

Of course, there are some cases in which assuming \textit{maximal} compositionality is a bit strict, and not consistent with what is actually going on. We will present solutions for these cases later.

\subsection{Intuition}

In words, the set of structures considered in this thesis can be characterised by the following properties:
\begin{enumerate}
\item The structures are projective trees.
\item The non-terminal nodes in the tree correspond to subsequences that were possible parts in the translation, while sibling terminal nodes toghether constitute a translation admissible part, herewith respecting the strategy of compositional translation.
\item Nodes can have both non-terminal nodes and terminal children.
\item All nodes of the tree have a minimal branching factor.
\end{enumerate}

\noindent The set of structures considered is thus associated with a synchronous context-free grammar, in which expansion into the most general non-terminals is preferred over expansions into words, and phrasal translations are used only when necessary. This captures the intuition that even in complicated cases like syntactic divergence, the parts constituting the most important elements of the sentence are grossly the same, and the difference in syntactic structure is reflected only in a few words. We will provide some examples to elucidate this.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\draw node (I) at (0,0) {I};
\draw node (dont) at (0.7,0) {don't};
\draw node (like) at (1.5,0) {like};
\draw node (cars) at (2.2,-0.05) {cars};

\draw node (Je) at (5.0,0) {Je};
\draw node (n) at (5.45,0) {n'};
\draw node (aime) at (6.0,-0.02) {aime};
\draw node (pas) at (6.8,-0.07) {pas};
\draw node (les) at (7.4,0) {les};
\draw node (voitures) at (8.3,-0.01) {voitures};

\coordinate (Je_) at (5.0,0.4);
\coordinate (like_) at (1.5,0.3);
\coordinate (I_) at (0,0.4);
\coordinate (aime_) at (6.0,0.4);
\coordinate (naimepas) at (6.0,1);
\coordinate (dontlike) at (1.2,0.8);
\coordinate (Idontlike) at (0.6,1.3);
\coordinate (Jenaimepas) at (5.6,1.4);
\coordinate (lesvoitures) at (7.8,0.2);
\coordinate (all) at (1,1.8);
\coordinate (tout) at (6.6,2.2);
\coordinate (n_) at (5.45,-0.2);

\foreach \coordinate in {dontlike, like_, I_, Je_, aime_, Idontlike, all, tout, Jenaimepas, naimepas}
	\filldraw (\coordinate) circle (0.035);

\foreach \from/\to in {naimepas/aime_, naimepas/n, naimepas/pas, dontlike/like_, dontlike/dont, Idontlike/I_, Idontlike/dontlike, all/Idontlike, all/cars, Jenaimepas/Je_, Jenaimepas/naimepas, tout/Jenaimepas, tout/lesvoitures}
	\draw (\from) -- (\to);

\foreach \from/\to in {cars/voitures, cars/les}
	\draw[<->, bend left = -35, thick, blue] (\from) to (\to);

\foreach \from/\to in {dont/n_, dont/pas, tout/all, Jenaimepas/Idontlike, aime_/like_, Je_/I_}
	\draw[<->, bend left = -35, thick, green] (\from) to (\to);

\draw[<->, bend left = -35, thick, red] (I) to (Je);

\end{tikzpicture}
\caption{Translation of negation, French-English}\label{fig:nepas}
\end{figure} 


A famous example that is often taken as troublesome in MT (refs?) is the translation of (English) negation in the non-contiguous French `ne ... pas'. Figure \ref{fig:nepas} shows a compositional translation tree that accounts for such a translation in the system sketched in this thesis. The translation tree shows that `I dont like' is the translation of `Je n'aime pas', but also contains the information that `don't' is phrasally translated as `ne ... pas'. Removing the negation in the English sentence results in the grammatical English sentence `I like cars', removing its translation equivalent in the France sentence in its (almost) grammatical `Je aime les voitures'. 

An example containing syntactic divergence in translation between Russian and English is depicted in Figure \ref{fig:russian}. In Russian, `X has Y` is (communistically) translated as `with X is Y', the object in English is thus the subject in Russian. Figure \ref{fig:russian} shows how this is dealt with in a translation structure. Note that although this translation tree is easily extendible to longer sentences with the same construction. By expanding the non-terminal nodes it can also capture sentences like `the girl with the long blond hair has a very old car with broken windows'.

\begin{figure}[!ht]
\centering
\input{syntactic_linked_structures.tex}
\caption{Translation of possession, Russian-English}\label{fig:russian}
\end{figure}

\subsection{Representation and Generation}

To study HATs, we need to be able to generate the set of HATs for every sentence. As generating and storing all trees separately would be both time and space consuming, and would impede a flexible search through them, hence a suitable representation of the set of HATs is required. In this thesis, we will represent the set of HATs for a sentence implicitly by a context-free grammar, whose node labels correspond to the spans the respective node covers (anders). An example can be found in Figure \ref{fig:grammar}

\begin{figure}[!ht]\begin{framed}
\small{
\begin{tabular}{llllll}
(0-6] $\rightarrow$ (0-1]  (1-6] && (4-6] $\rightarrow$ (4-5]  (5-6] && (0-1] $\rightarrow$ My\\
(0-6] $\rightarrow$ (0-2]  (2-6] && (0-4] $\rightarrow$ (0-1]  (1-4] && (1-2] $\rightarrow$ dog\\
(0-6] $\rightarrow$ (0-4]  (4-6] && (0-4] $\rightarrow$ (0-2]  (2-4] && (2-3] $\rightarrow$ also\\
(1-6] $\rightarrow$ (1-4]  (4-6] && (1-4] $\rightarrow$ (1-2]  (2-4] && (4-5] $\rightarrow$ eating\\
(1-6] $\rightarrow$ (1-2]  (2-6] && (0-2] $\rightarrow$ (0-1]  (1-2] && (5-6] $\rightarrow$ sausages\\
(2-6] $\rightarrow$ (2-4]  (4-6] && (2-4] $\rightarrow$ (2-3] likes\\
\end{tabular}
\caption{The grammar generating the translation tree forest for the sentence
`My dog also likes eating sausages', with set-permutation $\langle _0\{0\}_1,~ _1\{1\}_2,~ _2\{3\}_3,~ _3\{2,4\}_4, ~_4\{6\}_5,~ _5\{5\}_6\rangle$ would be (the subscripts indicating the span annotation, which is left exclusive and right inclusive)}\label{fig:grammar}
}
\end{framed}
\end{figure}

The allowed expansions were found with an algorithm similar to \citepos{dijkstra1959note} shortest path algorithm (Algorithm \ref{alg:shortest paths}), that was adapted to be more efficient given the extra knowledge of the alignment graph.

\begin{algorithm}[!ht]
\caption{Shortest Paths}\label{alg:shortest paths}
\begin{algorithmic}
\STATE \textbf{Input:} A graph $G = (V,E)$ describing an alignment and two vertices $i$ and $j$ for which $(i,j)\in E$ is true.
\STATE \textbf{Output:} All non-trivial shortest paths from $i$ to $j$
\STATE \textit{\#Initialization}
\STATE visited = $\emptyset$, depth = $0$, paths = $\{j\}$
\STATE $\forall n\in\mathbb{N}:$ reachable(n) = $\emptyset$; reachable($0$) = $\{j\}$
\STATE depth\_finished = False
\STATE \textit{\# Start backwards search through graph}
\WHILE{not depth\_finished or $i\notin$ visited}
	\WHILE{reachable(depth) $\neq\emptyset$}
		\STATE depth\_finished $\leftarrow$ False
		\STATE current\_node $\leftarrow N$ an arbitrary element $v$ from reachable(depth)
		\STATE reachable(depth) $\leftarrow$ reachable(depth) $-$ $\{$current\_node$\}$
		\FOR{ ($l$,current\_node) $\in E$}
			\IF{$l\notin $visited $\cup$ reachable(depth) \AND depth $\neq 0$}
				\STATE reachable(depth+1) $\leftarrow$ reachable(depth+1) $\cup$ $\{l\}$
				\FOR{path (current\_node,\ldots, $j) \in$ paths}
					\STATE path $\leftarrow$ ($l$,current\_node,\ldots, $j)$
				\ENDFOR
			\ENDIF
		\STATE visited $\leftarrow$ visited $\cup$ $\{l\}$
		\ENDFOR
	\STATE depth\_finished $\leftarrow$ True
	\STATE depth $\leftarrow$ depth+1
	\ENDWHILE
\ENDWHILE
\STATE \textbf{Return} paths
\end{algorithmic}
\end{algorithm}


\section{Monolingual Compositionality}

Intro, we use dependency parses blablabla. In the previous chapter, we have defined dependency grammars and parses in a very general fashion. We have already defined dependency parses in the previous chapter, and will not do that again.



Following common practice when using dependency structures, in this thesis it is assumed the dependency structure satisfies the following two conditions:\begin{enumerate}
\item When seen as a relation, $D$ constitutes a single-headed a-cyclic graph in which the words in $s$ are the nodes. (tree-constraint)
\item When the words are placed in the original order, the branches of the dependendency tree do not cross. (projectivity)
\end{enumerate}


\subsection{Representation}

We will represent a dependency parse as a set of relations, as is expressed in the following definition:\footnote{Note that this definition is different from the def....}

\begin{definition}[Dependency structure]
A dependency structure of a sentence $s = w_0\cdots w_n$ is a set of dependencies $D = \{ (i,j) |$ there is a dependency arrow from word $w_i$ to word $w_j \}$. 
\end{definition}

Formally, dependency grammars are not interpreted as compositional grammars, as they do not postulate the existence of non-terminal syntactic categories and therefore do not explicitly specify how a sentence was bult up from its parts. However, dependency graphs do give rise to an hierarchical structure, that specifies from which smaller parts the sentence was composed. For instance, the dependency graph depicted in Figure \ref{fig:deptree1} tells us that `likes' is the head word of the sentence, and that the sentence is composed of 4 parts: the head `likes', its modifier `also', its noun subject whose head is `dog' and the open clausal complement whose head is `eating'. The complement and subject are further divisible in `My' and `dog', and `eating' and `sausage', respectively. As the tree is projective, all parts are continuous. Also the graph in Figure \ref{fig:depgraph} prescribes an hierarchical structure: it is composed of the subject `I', the headword `know', and the phrase headed by `liet', that is in its turn built up from its head `liet', `dat', `hij' and the discontinous phrase `me winnen'. Such an hierarchical structure can not be captured by a phrase structure grammar.



\begin{figure}[!h]\label{fig:deptree1}
\centering
\begin{dependency}[theme=simple]%[hide label]
\begin{deptext}[column sep=.5cm, row sep=.1ex]
%PRP\$ \& NN \& RB \&[.5cm] VBZ \& VBG \& NN \\
My \& dog \& also \& likes \& eating \& sausage \\
\end{deptext}
\deproot{4}{}
\depedge{2}{1}{poss}
\depedge{4}{2}{nsubj}
\depedge{4}{3}{xvmod}
\depedge{4}{5}{xcomp}
\depedge{5}{6}{dobj}
\end{dependency}
\caption{Stanford Dependency Tree}
\end{figure}

\subsection{Generation}

To assign dependency structures to sentences, we used the Stanford Dependency Parser, that can be downloaded from their website, as well as used online  \citep{de2006generating}. The parser provides 5 variants of a typed dependency representation, of which the most basic one corresponds to the earlier imposed conditions on dependency structures. A picture of a dependency parse that was  generated by the parser is depicted in Figure \ref{fig:deptree1}. The Stanford Dependency parser does not include relations between words and punctuation, which is suboptimal, as they occur a lot in blabla. We will later explain how the problems arising from this fact were dealt with.

\section{Combining HATs and Dependency Parses}

%zeg anders, meer belangrijk, meer gerelateerd aan het grote plaatje: de vraag in deze thesis
To combine dependency parses and translation structures, means need to be find to quantify consistency and similarity between them. We have arrived at the situation depicted in Figure \ref{fig:depshats}.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}

\coordinate (ss) at (1.5,0);
\node [below] at (ss) {dependency parses};

\draw[->,bend right = 55,thick] (1.47,0) to (0,0);
\draw[->,bend left = 55, thick] (1.53,0) to (3,0);
\draw[->,bend left = 55, thick] (0.0,0) to (1.2,0);
\draw[->,bend right = 55,thick] (1.12,0) to (0.5,0);
\draw[->,bend left = 55, thick] (1.53,0) to (2.0,0);
\draw[->,bend right = 55,thick] (2.9,0) to (2.3,0);

\coordinate (ts) at (7.5,0);
\node [below] at (ts) {compositional translation structures};

%\draw (6,0) -- (0.6,2) (9,0) -- (6.6,2);
\draw (6,0) -- (6.9,2) (9,0) -- (6.9,2);
\draw (6,0) -- (7.2,2) (9,0) -- (7.2,2);
%\draw (6,0) -- (7.5,2) (9,0) -- (7.5,2);
\draw (6,0) -- (7.8,2) (9,0) -- (7.8,2);
\draw (6,0) -- (8.1,2) (9,0) -- (8.1,2);
\draw (6,0) -- (8.4,2) (9,0) -- (8.4,2);

\coordinate (startarrow) at (3.1,0.7);
\coordinate (endarrow) at (5.9,1.2);
\node (t) at (4.4,1.5) [above]{consistency?};

\draw[<->,bend left =35, thick] (startarrow) to (endarrow);

\end{tikzpicture}
\caption{New situations: finding consistency between dependency parses and compositional translation structures}\label{fig:depshats}
\end{figure}

Explain how it is not immediately clear how dependency parses and translation structures: when they are the same they should get an optimal score, and when they are really different not, even if some parts are maybe still the same

explain different kinds of similarity

\subsection{Definitions of Consistency}

\subsection{Based on Labels}

Explain based on labels, F-score like, explain why not suitable (give example preferably!)

\subsection{Based on Relations}

Still several ways this can be done. Definitions based on whether a relation is respected by the HAT

\subsection{Scoring Trees}

\section{Implementation}

Say something about the fact that the real implementation is documented and available ... give some details as to how it was found.


\bibliography{thesisDH}
\end{document}