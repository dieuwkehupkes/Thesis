\chapter{Background: an Overview of Machine Translation}

In this thesis, we address the universalities of language on an empirical level, through studying translation data. On some level, this thesis is thus related to linguistics in general. The research field to which this thesis is closest, however, is machine translation (MT). Not only do we build on previous empirical research conducted in the MT world, we also use the corpora, techniques and tools from this field, and our results are closely linked to - as well as interesting for - a specific type of MT models: structure based models. This chapter, in which the MT research field is discussed, serves several purposes, which we will set out in the following two paragraphs.

The chapter is divided into two parts. In the first section, an overview of MT is presented. We describe the developments in the field since the very first attempts, exemplifying the apparent cycle in the types of models that were investigated. Besides serving as a background sketch that helps the reader to put the thesis in perspective, this overview is meant to help the reader understand why all current state of the art MT models are structure based, and thus motivates the relevance of gaining a better understanding of such models. We also hope, that an overview of the various approaches tried in MT will help the reader appreciate the difficulty of the field, as well as its current state. This chapter is not meant as a thorough and complete overview of everything that has happened in MT over the years. The field is enormous, and discussing all this elaborately would not serve the purpose of this thesis. For instance, neither decoding nor technical implementation details of the models are discussed at all, as they are irrelevant for an empirical analysis of compositionality. For more elaborate overviews of MT, \cite{hutchins1992introduction} (early MT), \cite{somers1999review} (exemplar based MT), \cite{koehn2008statistical} or \cite{wu2005mt} (statistical MT) can be consulted, works that have all been used as references to write this chapter.

In the second part of this chapter (Sections \ref{sec:IBM} to \ref{sec:bcf}), the most important models in the history of MT are discussed in more detail (in the order they were presented in the overview). These sections provide a better insight in the many approaches that were investigated in MT, and therewith further emphasize why current MT systems - that will be discussed in the last two of these sections - are in their current state. Furthermore, they will introduce the reader to the techniques used in these models and the ideas on which they are founded. Many of these techniques and ideas are still very important for MT in general, and this thesis in particular.

As the current chapter provides a summary and literature study, but does not present any original work, it might be superfluous for readers with an extensive knowledge of MT. These readers might leap to the end of this chapter, were a summary of the most important points is provided.

\section{A brief History of Machine Translation}
\label{sec:overview}

Machine Translation arose as a research field almost immediately after the emergence of the first computers. In these early days, several different approaches were explored. These approaches can be roughly divided up into direct translation models and structure based translation models, which we will discuss in the next two paragraphs.

\myparagraph{Direct Translation}
In one branch of research, translation was approached as an encoding problem. Such models with a direct approach to translation are now known as the first generation models. Sentences were translated more or less word for word using some contextual information. Figure \ref{fig:georgetown} contains an example of the translation of the words `much' and `many' into Russian, according to one of the early systems \citep{dostert1955georgetown}.

\begin{figure}[!ht]
\begin{framed}
\footnotesize{
\begin{enumerate}
\item[1] Is the preceding word \textit{how}? (yes $\rightarrow$ \textcyr{skol\char126 ko}, no $\rightarrow$ 2)
\item[2] Is the preceding word \textit{as}? (yes $\rightarrow$ \textcyr{stol\char126 ko qe}, no $\rightarrow$ 3)
\item[3] Is current word \textit{much}? (yes $\rightarrow$ 5, no $\rightarrow$ 6)
\item[4] Not to be translated
\item[5] Is preceding word \textit{very} (yes $\rightarrow$ 4, no $\rightarrow$ \textcyr{mnogo})
\item[6] Is preceding word a preposition, and following word a noun? (yes $\rightarrow$ \textcyr{mnogii}, no $\rightarrow$ \textcyr{mnogo})
\end{enumerate}
}
\end{framed}
\caption{Translation of much and many from English to Russian in a direct translation system \citep[source][p.56]{hutchins1992introduction}.}\label{fig:georgetown}
\end{figure}

\myparagraph{Structure Based Translation}
A second line of research followed a more theoretical approach, involving fundamental linguistic research. The models of this strand targeted at translating language through using its underlying compositional structure. The most ambitious approaches aimed at finding representational systems, called interlingua, to express meaning on an abstract level and translate via such abstract meaning representations (thus finding an abstract meaning representation for the source text and generating a target text from this representation). The results of such models were disappointing, and the general consensus in the MT community was that the less ambitious transfer approach, in which different structural representations for source and target language were used, had the best prospects for significant improvements. In such transfer models, an extra stage is added to the process: the source text is first analysed into a structural representation containing information about the meaning and structure of the text and then this representation is mapped to a target side representation from which a target text can be generated. An example of a transfer rule used in the system Ariane \citep{boitet1982implementation}, is depicted in Figure \ref{fig:transferex}.

\begin{figure}
\begin{framed}
\centering
\begin{tabular}{m{5cm}m{1ex}m{5cm}}
\Tree [.\textit{supply} [.ARG0\\subj A ] [.ARG1\\obj B ] [.ARG2\\prep-obj \textit{with} C ] ] & $\mathbf{\Longrightarrow}$ & \Tree [.\textit{fournir} [.ARG0\\subj A ] [.ARG2\\i-obj B ] [.ARG3\\d-obj C ] ]\\
\end{tabular}
\end{framed}
\caption{Transfer rule that accounts for the translation of `A supplies B with C' into `A fournit C \`{a} B' \citep[source:][p.230]{hutchins1992introduction}.}\label{fig:transferex}
\end{figure}

Although the early systems were sometimes successful in small subdomains of language (e.g., \cite{chandioux1976meteo} for meteorolocial forecasts), they failed to scale to bigger domains, as it is very hard to formalize all of language in one system. Driven by exactly this thought, a new line of research came of the ground, that was not primarily based on linguistic knowledge, but on large pairs of texts that were translations of each other.\footnote{Such parallel texts were not created for this purpose, but exist by the grace of multilingual societies. Parallel corpora can be created from, e.g., proceedings of governments that are kept in two languages, websites that are translated into multiple languages, and news organisations that publish their articles for a multilingual audience \citep{koehn2008statistical}. Techniques to align these text on the sentence level \citep[e.g.,][]{varga2007parallel} have made such parallel texts very valuable for MT research.} Corpus based models can be roughly divided into exemplar based models and statistical models, although exemplar based models do not necessarily exclude the use of probabilistic techniques.

\myparagraph{EBMT}
The keystone of exemplar based machine translation (EBMT), is the direct use of analogy. Sentences are translated by finding examples in the corpus similar to (fragments of) the sentence, and recombining them into a translation. Although most MT models are in some sense based on this general idea, the match-and-change method as typically used in EBMT is computationally not scalable, as huge databases need to be searched for similar phrases or sentences, and similarity is not easy to establish (nor define). To exploit the systematicities in translation of natural language, more sophisticated methods are required, that are often probabilistic or statistical in nature.


\myparagraph{Statistical Word-based Models}
The models called `statistical models' take another perspective. Rather than basing translation on examples found in the corpora, they use the corpora to statistically decide on the parameters of another (mathematical) model. The first working statistical models \citep{brown1988statistical,brown1990statistical,brown1993mathematics} were ground-breaking. Although these models were intrinsically word-based, the quality of their translations was an enormous improvement over that of any earlier model. Equally important is, that the models, now known as `IBM model 1-5', were able to output a translation for any given input sentence (even ungrammatical ones). The statistical framework takes the view that every sentence $t$ is a possible translation of every sentence $s$. Modelling translation thus consists of modelling the probability $P(t|s)$ that $t$ is a translation of $s$, and finding the sentence $t$ for which this probability is highest. The probability distribution is statistically learned from the parallel corpus. A more detailed description of the IBM models, as well as information on how they deal with phenomena as reordering, can be found in Section \ref{sec:IBM}.

\myparagraph{Statistical Phrase-based Models}
The statistical IBM models still had the same drawbacks as the first generation of direct translation models: no structure or local context was considered, and a large amount of natural language phenomena could therefore not be accounted for. With the introduction of phrases as basic units in translation models \citep{wang1998grammar,och1999improved} a major leap forward was taken towards a proper treatment of these problems. A phrase translation pair is a pair of contiguous source and target sequences such that the words in the source phrase are aligned only with words in the target phrase, and vice versa \citep{och2000improved}. Phrases are thus not restricted to linguistic phrases, but can be any arbitrary contiguous sequence of words whose translation constitutes a contiguous sequence in the target sentence. Phrase-based translation models can therefore capture short contiguous idomatic translations, as well as small insertions and deletions and local reordering. E.g., both `a casa' and `o casa' are reasonable word for word translations of the English phrase `the house'. However, `o casa' is not a grammatical string in Portuguese. The latter observation could be easily captured by a phrase-based model, as `the house' could be translated as one unit, but would be much harder to model in a word-based model. Furthermore, a word-based model would never be able to get the correct idiomatic translation of `bater as botas' (the Portuguese equivalent of `to kick the bucket'), while a phrase-based model would have little trouble finding this translation, provided this specific idiomatic phrase was present in the training corpus. Phrase based models are discussed in more detail in Section \ref{sec:pbmodels}.

\myparagraph{Statistical Structure based Translation Models}
Phrase based models, although still considered state of the art, suffer from the fact that no structure beyond the phrase level is taken into account. Approaches that addressed this problem by incorporating syntactic information to, e.g., sophisticate phrase selection of a standard phrase-based system \citep{koehn2003statistical} or rerank its output \citep{och2004alignment} were not very successful, which lead a large part of the MT community to move back to models similar to the earlier transfer based models that ruled the field before the emergence of statistical models. The newer transfer models stayed true to the statistical and corpus based tradition, in which translation is formulated as learning a probability distribution from a parallel corpus. While the rules in the old transfer models were constructed manually, the rules new batch of transfer models were based on the patterns found in translation corpora, making the models more scalable and robust (but leading to more limited mappings than before, which we will discuss in more detail in Section \ref{sec:SCFGs}).

%\begin{figure}
%\centering
%\begin{tikzpicture}
%
%\coordinate (ss) at (1.5,0);
%\node [below] at (ss) {source sentence};
%
%%\draw[] (0,0) -- node[below]{source sentence} (3,0);
%\draw (0,0) -- (0.6,2) (3,0) -- (0.6,2);
%%\draw (0,0) -- (0.9,2) (3,0) -- (0.9,2);
%\draw (0,0) -- (1.2,2) (3,0) -- (1.2,2);
%\draw (0,0) -- (1.5,2) (3,0) -- (1.5,2);
%%\draw (0,0) -- (1.8,2) (3,0) -- (1.8,2);
%\draw (0,0) -- (2.1,2) (3,0) -- (2.1,2);
%\draw (0,0) -- (2.4,2) (3,0) -- (2.4,2);
%
%\coordinate (ts) at (7.5,0);
%\node [below] at (ts) {target sentence};
%
%%\draw (6,0) -- (0.6,2) (9,0) -- (6.6,2);
%\draw (6,0) -- (6.9,2) (9,0) -- (6.9,2);
%\draw (6,0) -- (7.2,2) (9,0) -- (7.2,2);
%%\draw (6,0) -- (7.5,2) (9,0) -- (7.5,2);
%\draw (6,0) -- (7.8,2) (9,0) -- (7.8,2);
%\draw (6,0) -- (8.1,2) (9,0) -- (8.1,2);
%\draw (6,0) -- (8.4,2) (9,0) -- (8.4,2);
%
%\coordinate (startarrow) at (3.1,1.2);
%\coordinate (endarrow) at (5.9,1.2);
%
%\draw[<->,bend left =35, thick] (startarrow) to (endarrow);
%
%\end{tikzpicture}
%\caption{A graphical representation of compositional translation, or tree-based transfer translation.}\label{fig:comptrans}
%\end{figure}

Translation structures, however, are yet another hidden component in translation data. They need to be inferred (or learned), which is not a trivial task. Some approaches tried to base the structures involved in translation on monolingual syntactic structures, by first parsing the source and target sentence into such a structure, and try to establish correspondences between their nodes. This so called `parse-match-parse' method has a couple obvious limitations. Fully automated parsers that can parse large amounts of text efficiently are required for both source and target language, which extremely limits the number of language pairs that can be treated with this approach. Furthermore, monolingual grammars are not designed for translation purposes, and there is no guarantee that source and target structures are similar enough to find correspondences for every part of them. How suitable monolingual syntax is for translation is a genuine question, that lies at the heart of this thesis, and will thus be discussed extensively in later chapters.

A second approach is to forget about linguistic information, and concentrate on the structures suggested by the translation data. Models using this strategy are based solely on alignments (see \ref{sec:alignments}), that describe which source words are translated into which target words, and the restrictions they impose on structural representations of the sentence (more details are provided in the next chapter). As alignments generally give rise to a huge number of structures for every sentence, models using this method are hindered by computational issues. Several different solutions to restricting the structure space have been presented, in some of which formal criteria were used, while in others linguistic information was incorporated. We will discuss these methods in Section \ref{sec:SCFGs}. In this section we will also pay more attention to the practical side of such models.

%Section title
\section*{Summary}
We have given a brief overview of machine translation from the very start until now, that started with the direct first generation models, and ended with the structure based statistical models that are considered in this thesis. Before we will discuss the latter models in detail, we will first provide a more elaborate description of statistical word- and phrase based models (in Section \ref{sec:IBM} and \ref{sec:pbmodels}, respectively). Statistical word- and phrase-based models lied the ground work for statistical transfer models, and a description of their techniques and ideas is thus indispensable to properly understand the rest of this thesis.

\section{IBM models}
\label{sec:IBM}

In this section, a explanation of the main concepts used in the word-based models presented by \cite{brown1993mathematics} will be provided. These models, that were the first working statistical models, laid the foundation for current state of the art translation models. The IBM-models focussed on learning the probability distributions $P(t|s)$ - the probability that a target sentence $t$ is a translation of a source sentence $s$ - from a parallel corpus. That is, the predefined model for $P(t|s)$ has parameters that can be estimated from the translation data.\footnote{For instance, $P(t|s)$ might be dependent on the probability distribution of the different translations of the word `obvious' in the corpus.} The hope is, that the learned distribution predicts translations in line with human intuitions. For instance, $P(t|s)$ should be high for ($t,s)$ = (`I grow chilli plants in my backyard', `ik kweek chili plantjes in mijn achtertuin'), and low for ($t,s$)= (`I grow chilli plants in my backyard',`gisteren is mijn portemonnee gestolen').

\subsubsection{Modelling $\mathbf{P(t|s)}$}
\citeauthor{brown1988statistical} use Bayes' rule to split the translation probability into multiple probability distributions, yielding the following expression:

\[
P(t|s) = \frac{P(t)P(s|t)}{P(s)}
\]

As $P(s)$ does not depend on $t$, this results in the following equation (called `The Fundamental Equation of Machine Translation' by the authors) for the desired translation $\hat{t}$:

\[
\hat{t} = \operatorname*{arg\,max}_t P(t)P(s|t)
\]

The equation splits the translation task in two: modelling the translation probability $P(s|t)$, and modelling the language probability $P(t)$. In \cite{brown1993mathematics}, 5 different models of increasing complexity are presented to model the translation probability. These models are generally referred to with the names `IBM models 1-5'. In all these models, the probability $P(s|t)$ is modelled by marginalizing over all possible ways in which the words in $t$ could have been generated by the words in $s$, which is expressed by an alignment function $a$ (more information on which can be found in Section \ref{sec:alignments}). Thus: $P(s|t) = \sum_a P(s,a|t)$. $P(s,a|t)$ cannot be computed exactly, and the 5 IBM models differ in the complexity of their approximation. For instance, in IBM model 1, all alignments are assumed to have an equal probability, and the probability $P(s,a|t)$ is the (normalized) product of all the lexical translation probabilities $p(s_j|f_{t(j)})$ indicated by the alignment. The translation probabilities for sentences $t$ with the same words in different orders are thus identical. To address this issue, an alignment probability distribution is added in IBM model 2. In later models, also fertility of the input words is considered, and also the word-order differences between source and target language are modelled more sophisticatedly. In Figure \ref{fig:IBM-model} an example of IBM-style translation is depicted. 

The parameters of the IBM models (e.g., lexical translation probabilities, fertilities of the words, alignment probabilities) are learnt from a parallel corpus using the expectation maximization algorithm \citep{dempster1977maximum}, on which a short explanation can be found in Section \ref{sec:alignments}. Mathematical details on the exact procedure of parameter estimation for the IBM models can be found in \cite{brown1993mathematics}.

\begin{figure}[!ht]
\begin{framed}
\scriptsize{
Consider the following pair of sentences and a possible alignment (the numbers indicate the alignment: (Le chien e battu per Jean, John (6) does beat (3,4) the (1) dog (2)). The probability $P(s,a|t)$ is computed as follows:\begin{enumerate}
\item Compute the lexical probabilities of the source words being translated into the target words, thus compute: $P(Jean|John)\cdot P(est|beat)\cdot P(battu|beat)\cdots$
\item factor in the fertility probabilities of the source words, thus multiply with:  $P(f\!=\!1|John)\cdot \cdot P(f\!=\!1|does) \cdot P(f\!=\!2|beat)\cdots $
\item Factor in the distortion probabilities, that are in this model just depending on source and target position and target length, thus multiply with: $P(1|4,l\!=\!6)\cdot P(2|5,l\!=\!6)\cdot P(3|3,l\!=\!6) \cdots $
\end{enumerate}
The parameters for this IBM model are thus: a set of lexical translation probabilities $P(f|e)$, a set of fertility probabilities $P(n|e)$ and a set of distortion probabilities $P(i|j,l)$ for each target position $i$, source position $j$ and target length $l$. In practice, $i,j,l$ and $n$ are maximally $25$.
}
\end{framed}
\caption{Example from \cite[p.3]{brown1990statistical}, that shows the workings of the IBM word-based translation model}\label{fig:IBM-model}
\end{figure}


\subsubsection{Modelling $\mathbf{P(t})$}
The language model, a probability distribution for $P(t)$, is supposed to account for fluency and grammaticality of the target language string. That is, to prevent the model from putting too much probability mass on not well formed target strings. In the IBM models, the probability distribution is an $n$-gram model, whose parameters can be estimated through relative frequency estimation on the target side of the parallel corpus. The set-up in which a separate language model is used to assign probabilities to translations is used by almost every current state of the art MT-model.


\section{Phrase-based models}
\label{sec:pbmodels}

Phrase-based models address some of the limitations of word-based models, by using phrases instead of words as basic units in the translation models, which allows the translation model to take into account local context. For instance, the translation of a sentence with a simple phrase based model could be executed by the following described sequence of steps. The foreign sentence is first broken up into phrases. These phrases are then translated as a whole, and the probability of the source sentence given the target sentence\footnote{A quick reminder: the generative model of phrase-based models is largely similar to the word-based IBM models, the translation probability is still inverted due to application of Bayes' rule.} is defined as the product of the phrasal translation probabilities and a `distortion' probability based on how far every phrase was moved relatively to the previously translated phrase \citep{koehn2003statistical}. The probability of the target sentence given the source sentence can then be computed taking into account the language model.

To translate with phrases, a phrase translation table is needed in which probabilities are assigned to the translation of source phrases in target phrases. Phrase-tables can be acquired in different ways \citep{marcu2002phrase,och1999improved,koehn2003statistical,mylonakis_simaan_emnlp_2008}, details of which are not relevant to this thesis. An aspect that is more relevant to this thesis relates to the `definition' of a phrase -  in other words: when can a subsequence of a sentence pass for a phrase that can be used in translation - which will be discussed in Chapter \ref{ch:empirical}.

Phrase-based translation has some obvious advantages over word-based translation. First of all, short idiomatic translation can be accounted for in an intuitive fashion: directly assigning a probability to `of course' as the translation of `natuurlijk' makes intuitively more sense than having two separate entries that assign probabilities to `of' and `course' being translations for `natuurlijk'. Secondly, phrases can use local context, which means they can make informed decisions about the translation of, e.g., the gender of determiners and adjectives, an example of which was given in the overview in Section \ref{sec:overview}. Finally, phrases can capture local reordering phenomena of phrases seen during training, making it easier to prefer `the Italian woman' over `the woman Italian' as translation of `la donna italiana'.

Phrase based models also have certain limitations, of both practical and theoretical nature. Firstly, phrase based models have no clear strategy to account for global reordering. Due to data sparsity, useful phrases are generally not much longer than 3 words, and can thus not account for such reordering phenomena on their own. Naturally, reordering models are included in most phrases based models, but the reordering space for phrases is, although significantly reduced with respect to word-based models, still too large to exhaustively search through all possibilities, and phrasal movement is generally only considered within a window of a couple of phrases. Global reordering of entire constituents that take up several phrases can thus not be captured with such reordering models.

A second issue concerns the partitioning of the sentence into phrases. The probability of this partitioning is rarely considered, and phrases are not allowed to overlap, resulting in poor modelling of agreement phenomena. 

Another difficulty with phrase based models arises in the assignment of probabilities to phrase pairs, which is, as mentioned before, not straight forward. Several approaches have been used to learn phrase-translation tables \citep[see][p.130]{koehn2008statistical}. 

Finally, phrase-based models have no means to detect systematicities in phrase structures that can help them to generalise beyond what they have seen in the training data. Even if a phrase based system has seen several examples similar to `la donna italiana' (article adjective noun), it will not infer that adjectives and nouns in translation between Italian and English generally switch order.\footnote{Of course there are many situations in which phrases still can be used to capture this implicitly. If an unknown adjective-noun combination is to be translated (say `la donna tedesca'), the model does not have to fall back on word for word translation, but can (in this case) combine the phrases `la donna' and `tedesca ...'.}

\section{Synchronous Context Free Grammars}
\label{sec:SCFGs}

To address the global reordering problem, more structure needs to be incorporated, which prompted the revisiting of transfer models. To statistically exploit transfer methods, syntactic formalisms for both source and target side are needed, as well as a method of combining them. The transfer process in statistical based transfer models differs slightly from the previously sketched picture, in which entire source structures were mapped to entire target structures. As a rule, the source and target structures (or sentences) are assumed to be generated simultaneously (bit by bit) by two `linked' monolingual grammars, that are together called a synchronous grammar.
 Figure \ref{fig:syncgen} depicts an example of the simultaneous generation of a sentence and its translation with a synchronous (context free) grammar. Regarding the complexity of the monolingual grammars, there are several choices that can be made. Some translation models have incorporated relatively simple formalisms as finite state machines \citep[e.g.,][]{alshawi2000learning}, others relatively heavy formalisms as tree adjoining grammars (e.g., \cite{poutsma2000data} based a translation model on DOP). However, the lion's share of the statistical tree based transfer models uses synchronous context free grammars (SCFG's), and even approaches that are not explicitly concerned with CFG's can often be reformulated as such.
 
\subsection{Formally}

An SCFG is a the synchronous extension of a CFG, as introduced by \cite{chomsky1956three}:

\begin{definition}[Context Free Grammar]
A context free grammar (CFG) is a quadruple $G = (V, \Sigma, R, S)$, where\begin{enumerate}
\item $V$ is a (finite) set of non-terminals, in the context of natural language often interpreted as syntactic categories.
\item $\Sigma$ is a (finite) set of terminals, corresponding to the lexical items of the language.
\item $R$ is a relation from $V$ to $V\cup\Sigma$, to be interpreted as a set of rewrite rules.
\item $S\in V$ is the start symbol of the grammar.
\end{enumerate}
\end{definition}

An SCFG \citep{aho1969properties} is a grammar linking two CFG's that share a set of non-terminals, describing how their expressions can be generated simultaneously. Parse trees generated by SCFGs are thus isomorphic on the non-terminal level (i.e., there is a bijective mapping between the non-terminal nodes of the trees). Formally, we have:

\begin{definition}[Synchronous Context Free Grammar]
A synchronous context free grammar (SCFG) is a quadruple $G = (V, \Sigma, R, S)$, where\begin{enumerate}
\item $V$ is a (finite) set of non-terminals, the syntactic categories of both languages.
\item $\Sigma$ is a (finite) set of terminals, constituted by the union of the terminal symbols of the two languages.
\item $R$ is a set of rewrite rules of the form $X\rightarrow\langle\gamma,\alpha,\sim\rangle$, $\gamma\in (V\cup\Sigma)^{*}$,  $\alpha\in (V\cup\Sigma)^{*}$ and $\sim$ a one-to-one and onto correspondence between the non-terminal symbols in $\alpha$ and $\gamma$.
\item $S\in V$ is the start symbol of the grammar.
\end{enumerate}
\end{definition}

SCFG's implicitly model large scale reordering phenomena and long distance dependencies, as the non-terminal sequences that are generated in a production do not necessarily have the same order. Figure \ref{fig:syncgen} includes a small examples of this: the noun and adjective swap order during the translation. Such swaps can also occur on larger scales and SCFG can account for quite complicated reorderings.

\begin{figure}
\begin{framed}
The sentence pair (`La donna italiana \`{e} bella', `The italian woman is beautiful') can be simultaneously generated by two linked context free grammars as follows:

\vspace{2mm}
{\footnotesize
\noindent Simultaneously generate two noun and verb phrases:
}
{\tiny
\begin{center}\begin{tabular}{m{2cm}m{1ex}m{2cm}} \Tree [.S [.NP ] [.VP ] ] & $\sim$ & \Tree [.S [.NP ] [.VP ] ] \end{tabular}\end{center}
}
{\footnotesize \noindent Simultaneously expand the noun phrases (note the reordering):\\
}
{\tiny \begin{center}\begin{tabular}{m{3.8cm}m{1ex}m{3.8cm}} \Tree [.S [.NP [.DT ] [.JJ ] [.NN ] ] [.VP ] ] & $\sim$ & \Tree [.S [.NP [.DT ] [.NN ] [.JJ ] ] [.VP ] ] \end{tabular}\end{center}}
{\footnotesize
\noindent Simultaneously expand the verb phrases:
}
{\tiny\begin{center}\begin{tabular}{m{4.5cm}m{1ex}m{4.5cm}} \Tree [.S [.NP [.DT ] [.JJ ] [.NN ] ] [.VP [.VBZ ] [.ADJP ] ] ] & $\sim$ & \Tree [.S [.NP [.DT ] [.NN ] [.JJ ] ] [.VP [.VBZ ] [.ADJP ] ] ] \end{tabular}\end{center}}

{\footnotesize \noindent Simultaneously expand all pre-terminals to obtain the sentence pair (which summarises 5 steps):}
{\tiny\begin{center}\begin{tabular}{m{5.1cm}m{1ex}m{5.1cm}} \Tree [.S [.NP [.DT The ] [.JJ italian ] [.NN woman ] ] [.VP [.VBZ is ] [.ADJP beautiful ] ] ] & $\sim$ & \Tree [.S [.NP [.DT La ] [.NN donna ] [.JJ italiana ] ] [.VP [.VBZ \`{e} ] [.ADJP bella ] ] ] \end{tabular}\end{center}}
\end{framed}
\caption{Synchronous generation of translation}\label{fig:syncgen}
\end{figure}

\subsection{The computational difficulties of SCFG's}

As is not new to complexer MT models, also SCFG's are severely hindered by computational problems. On of the most significant problems, which we will highlight because it has been a motivational factor in many algorithms for learning SCFG's \citep[e.g.,]{zhang2006synchronous,zhang2008extracting,huang2009binarization} relates to the rank of an SCFG. The rank of an SCFG can be defined as the highest number of non-terminals occurring on the right hand side of a rule in the grammar in a single dimension \citep{gildea2006factoring}, which is also called the rank of a rule. For instance, the toy rule `NP $\rightarrow$ $\langle$ Det NN Adj, Det Adj NN $\rangle$' has rank 3, as three non-terminals are generated in both source and target language. As with monolingual CFG's, parsing with SCFG's is much more efficient if all the rules are binary (thus the rank of the grammar is 2). Contrary to monolingual CFG's, the rank of an SCFG cannot always be reduced to two by rewriting the rules. In other words: synchronous CFG's can not always be binarised \citep{huang2009binarization}, which makes parsing very inefficient.
%Discuss more difficulties?


\subsection{Learning SCFG's}
\label{subsec:learningSCFGs}

Learning an SCFG from translation data is a far from trivial task, which is amplified by the fact that bilingual data often do not coincide with monolingual linguistic structures. That is, trees generated by SCFG's are isomorphic on the non-terminal level, which is often not the case if the source and target trees are independently generated by two separate non adjusted parsers. Furthermore, the parts considered constituents by a monolingual CFGs are not necessarily translation admissible parts according to the translation data. This is not to say, that monolingual linguistic syntax is useless for MT. In fact, its usability is precisely what is addressed in this thesis. However, to stay on the topic of synchronous grammars, translation models solely relying on monolingual syntactic structures generally exceed the power of SCFG's. We will briefly discuss such models in Section \ref{sec:bcf}.

\subsubsection{Rule Extraction}

Learning an SCFG consists of determining its rules. In almost all working SCFG models, the grammar rules are induced from a parallel corpus by regarding the data as primary source of information (in contrast to using external, possibly linguistic, knowledge). This approach - introduced by \cite{wu1995algorithm} in the form of an inversion transduction grammar (ITG), that lies at the heart of many later approaches - is based on word-alignments and the constraints on structures (and thus rules) they prescribe. In Chapter \ref{ch:empirical}, we will take a closer look at alignments and the structures they induce. 

Purely data-driven models reinforce the computational problems of SCFG's, as the number of rules that can be extracted from a sentence grows exponentially with the length of the sentence \citep{quirk2006dependency}. Without considering additional information, there is no a priori reason to prefer one rule over another, yet some serious pruning of the rule space is necessary to make parsing computationally feasible. Furthermore, without linguistic information, a grammar naturally lacks non-terminal labels, which raises a new issue: inventing non-terminal labels. MT models differ in the number and kind of non-terminal labels that they use. In the remainder of this section we will briefly discuss two strategies that have been proposed to address the previously mentioned issues: the formal strategy, and the linguistic strategy. These strategies are not mutually exclusive, some models use them both. The explanation is meant to exemplify endeavours to address these problems, and although several references to models are provided it is not meant to give a complete overview of these. As before, we will not discuss the issue of assigning weights to the rules of the SCFG. Herewith we exclude the strategy of selecting rules by statistically learning which rules have a probability exceeding a certain threshold, an in many contexts very important part of the field. However, in the context of this thesis, knowledge of the different strategies in this area is not relevant, nor will it lead to a better understanding of the rest of this thesis.

\subsubsection{Restricting according to Rank}

A remedy that is often used to reduce the number of rules is to select only the rules whose rank does not exceed a certain maximum number. In most cases, the rules are restricted to binary (at most two right hand side non-terminals in one dimension), which reduces both the rule space and the parsing complexity \citep[e.g,][]{wu1997stochastic,chiang2005hierarchical,mylonakis2011learning}. This solution is computationally attractive, and easy to implement. 

Of course, the assumption that all of language can be captured in binary structures seems rather strong. \citeauthor{wu1997stochastic} claimed to be unable to find real-life examples of translations that could not be explained by such trees, but this was later refuted by others \citep[e.g.,][]{galley2004s}. However, the coverage of binary transduction grammars is still a hot issue in MT, to which we will come back later in Chapter \ref{ch:empirical}. 

Many of the models using this paradigm to prune the rule space do not really address the non-terminal label issue. \citepos{wu1995algorithm} ITG contains only a single non-terminal label, and his model thus merely learns a high-level reordering preference, without considering further contextual information. An improvement on this was presented by \cite{chiang2005hierarchical,chiang2007hierarchical}. Although also his grammar had no more than one non-terminal label, he allowed the right-hand side of his rules to contain both terminals and non-terminals, such that lexical information could be incorporated. An example of such a rule would be:

\[
\text{X } \rightarrow \langle\text{ X}_1 \text{ de X}_2 \text{, the X}_2 \text{ that X}_1\rangle
\]

\noindent which captures the fact that Chinese relative clauses modify noun phrases on the left, whereas English relative clauses modify on the right \citep{chiang2007hierarchical}.\footnote{To combine different non-terminals into a sentence, some more rules are needed. \cite{chiang2007hierarchical} adds the following two `glue rules' to his grammar:\begin{align*}
 \text{S } \rightarrow \langle\text{ S}_1\text{X}_2 \text{,S}_1\text{X}_2\rangle\\
\text{S } \rightarrow \langle\text{ X}_1 \text{,X}_1\rangle 
\end{align*}

}

The framework introduced by Chiang combines the strengths of rule-based and phrase-based models, and is referred to with the term `Hierarchical Phrase Based Translation'. To the knowledge of the author, there are, besides models following the previously mentioned probabilistic strategy of excluding rules \citep[e.g.,]{mylonakis2010learning,blunsom2008bayesian} no models that learn syntactic categories without invoking linguistic knowledge


\subsubsection{Incorporating Linguistic Information}

A sensible solution to address the previously mentioned issues with structure-based transfer models is to incorporate linguistic knowledge. Information from monolingual parsers can be used to, e.g., reduce the space of possible node spans, and to induce linguistically motivated terminal labels. Several researchers have followed this strategy, we will mention a few approaches. \cite{zollmann2006syntax} augmented a standard phrase-based grammar with syntactically motivated non-terminal labels, based on constituency grammars. \cite{almaghout2010ccg}, following \cite{hassan2007supertagged}, labelled phrase pairs with ccg based supertags \citep{steedman2011combinatory}. \cite{mylonakis2011learning} learned automatically which source-syntax labels fit best with a phrase-based CFG and the translation data.

Clearly, the number of language pairs that can be treated as such is vry limited, as it requires an automated linguistic parser (or other means of providing linguistic information on a large scale) for (at least one of) source and target language. However, using available syntactic or semantic knowledge can result in robust models that yet do not ignore or intuition of language, especially if high quality parsers are available.

\section{Beyond Context Free}
\label{sec:bcf}

Formally it is desirable to create grammars that generate isomorphic tree pairs for sentences that are each others translation, but there is no a priori reason that dictates that such structures exist. In fact, as CFG's have been proved to sometimes be inadequate to model certain natural language phenomena, more powerful transformation methods might be more suitable for the expressive syntactic transformations going on during the translation of natural language. As the necessity of deviating from conventional syntax is smaller, models of this class tend to stay closer to traditional linguistic structures.

\subsection{Synchronous Tree Substitution Grammars}

The class of Synchronous Tree Substitution Grammars (STSG's) is a strict superset of the class of SCFG's, and STSG's are therefore a natural extension to them. Models working with STSG's are, i.a., \cite{poutsma2000data} and \cite{galley2004s,galley2006scalable}. The core method of the former is to align chunks of parse trees of source and target sentences, and transform them into rules. \cite{poutsma2000data} requires the existence of a parallel corpus aligned on the subtree level. Such datasets were not available and the paper is merely a description of the STSG framework.  The model presented by \citeauthor{galley2004s} has a somewhat different set-up, learning rules to transform an source-language string into a target language tree. \cite{galley2006scalable} do provide an implementation, yielding promising results.\\
An approach that does not explicitly use STSG's, but whose grammar rules do exceed the power of CFG rules, is presented by \cite{melamed2004generalized}. In their generalized multitext grammar (GMTG) they let go of the requirement that constituents need to be contiguous, which allows them to synchronise languages generated by mildly context-sensitive languages. Also \citeauthor{melamed2004generalized} present a framework with suggestions for further work, rather than an implementation.

\subsection{Semantic Mappings}

The last category of models we will discuss, attempts to find mappings between more semantically oriented structures, that specify the predicate-argument structure of the sentence, that is often assumed to be somewhat universal. Such an approach is taken in \cite{menezes2003best}, in which transfer rules are extracted by aligning pairs of Logical Form structures. Another predicate-argument structure that is often used is the dependency parse (for more information on the dependency parse, see \ref{sec:depgram}), rules are inferred by either projecting or learning target-side paths. As such rules sometimes create or merge dependents according to the alignment, the dependency structures of source and target side need not be isomorphic, and such models can formally also be seen as STSG's \citep[as made explicit in][]{eisner2003learning}). Finding a mapping between two dependency trees is not only attractive because dependency trees represent the semantic structure of a sentence more closely than a constituency tree, but also because it is computationally more feasible, as dependency trees contain fewer nodes than constituency trees of the same sentence. Presented models differ in the linguistic plausibility of the target side dependency parse. E.g., \cite{eisner2003learning} learns mappings between two linguistic dependency trees (his article lacks a working implementation, although it does give a description of algorithms suitable for parsing with his model), while \cite{lin2004path}, extracts transfer rules that correspond to linear paths in the source side dependency tree, but not necessarily to linguistic dependency parses on the target side. The models presented in \cite{quirk2005dependency,quirk2006dependency,quirk2006we} also have clear dependency part, but employ several other strategies as well. They project source dependency trees to target dependency trees, following a set of rules, and extract from the resulting corpus a set of \textit{treelets} - arbitrary connected sub graphs - that are used in translation.

\section{Conclusion}

In this chapter, we sketched the background of this thesis. We presented a brief overview of the history of machine translation, and we discussed some of the relevant models and techniques in more detail. The reader should now be acquainted with the basics of statistical models of translation, know what alignments and phrase pairs are, and appreciate that to adequately model reordering and long distance dependencies, incorporation of information about the structure of the language is essential. We have also seen, that it is
 unclear how exactly the latter should be done. In particular, it became clear that the extent to which monolingual syntax is useful for such a process is unknown, and this question hence deserves further investigation.